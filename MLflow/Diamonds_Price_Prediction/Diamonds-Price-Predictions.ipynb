{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectice/vectice-examples/blob/master/Notebooks/MLflow/Diamonds_Price_Prediction/Diamonds-Price-Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through the Vectice integration with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Vectice, MLflow and GCS packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep the Vectice library lite, we install just the primary dependencies and let the user install the the other dependencies when needed. Here, we install github because our notebook is on Github and we are going to need the github package to be able to point to the notebook from the Vectice UI. You have to add the other dependencies (gitlab, bitbucket) if you're going to use them (!pip install -q \"vectice[github, gitlab, bitbucket]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxb_wbl9qsFC"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q vectice[github]\n",
    "!pip3 install -q fsspec\n",
    "!pip3 install -q gcsfs\n",
    "!pip3 install -q google-cloud-storage\n",
    "!pip3 install -q mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kYu_DFk5dMZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 show vectice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VztpAEtl5dMb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The main entrypoint of the SDK is the high level API which provide several solutions to follow your runs.\n",
    "\n",
    "* a procedural solution with 2 methods to call vectice.create_run() and vectice.save_after_run()\n",
    "\n",
    "* a more powerful solution based on vectice.Vectice class that provides itself several possibilities:\n",
    "\n",
    "* use an instance of vectice.Vectice object to create_run(), start_run() and end_run() (fluent API)\n",
    "\n",
    "* You can also use the context manager syntax (python with keyword): In this case, the end of the run will be automatically managed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuo0X3iPrEXn"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from math import sqrt\n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn import metrics\n",
    "\n",
    "import mlflow\n",
    "from vectice import Vectice\n",
    "from vectice.models import JobType\n",
    "from vectice.models.model import ModelType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwCGj5K06PM7"
   },
   "source": [
    "### Data:\n",
    "This classic dataset contains the prices and other attributes of almost 54,000 diamonds. There are 10 attributes included in the dataset including the target ie. price.\n",
    "\n",
    "### Feature description:\n",
    "\n",
    "price price in US dollars ($326--$18,823)This is the target column containing tags for the features. \n",
    "\n",
    "### The 4 Cs of Diamonds:-\n",
    "\n",
    "- carat (0.2--5.01) The carat is the diamond’s physical weight measured in metric carats.  One carat equals 1/5 gram and is subdivided into 100 points. Carat weight is the most objective grade of the 4Cs. \n",
    "\n",
    "- cut (Fair, Good, Very Good, Premium, Ideal) In determining the quality of the cut, the diamond grader evaluates the cutter’s skill in the fashioning of the diamond. The more precise the diamond is cut, the more captivating the diamond is to the eye.  \n",
    "\n",
    "- color, from J (worst) to D (best) The colour of gem-quality diamonds occurs in many hues. In the range from colourless to light yellow or light brown. Colourless diamonds are the rarest. Other natural colours (blue, red, pink for example) are known as \"fancy,” and their colour grading is different than from white colorless diamonds.  \n",
    "\n",
    "- clarity (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)) Diamonds can have internal characteristics known as inclusions or external characteristics known as blemishes. Diamonds without inclusions or blemishes are rare; however, most characteristics can only be seen with magnification.  \n",
    "\n",
    "### Goal: \n",
    "\n",
    "The goal is to predict the prices of diamonds using the features in the given dataset. Thus it's a regression problem, you'll perform a bit of data cleaning and create a multiple models that are fed into MLflow. The code used to achieve this is hiddin but you can view it. However, it'll be more fun to give it a good old college try as a team and resort to the hidden code if all else fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCo1DKnP5-Kh"
   },
   "source": [
    "Here is a link to the Python SDK Documentation, it's not final nor complete so you might need to troubleshoot a bit. \n",
    "[Python SDK Documentation](https://doc.vectice.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5fCsgSAAcfQ"
   },
   "source": [
    "Upload the GCS JSON. This is then declared as an environmental as seen below.\n",
    "\n",
    "```\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'test.json'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KRW2FrKr_qB"
   },
   "outputs": [],
   "source": [
    "# In Google Collab you can upload the json file that has your Google Cloud Service account details, with the following widget. This is used to access the data needed to perform the steps in the notenook.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrrgP1jT5dMh"
   },
   "source": [
    "## Vectice Credentials \n",
    "\n",
    "To connect to the Vectice App through the SDK you'll need the Project Token, Vectice API Endpoint and the Vectice API Token. You'll find all of this in the Vectice App. The Workspace allows you to create the Vectice API Token, in Projects you'll be able to get the Project Token, as seen below. The Vectice API Endpoint is 'https://beta.vectice.com'. You're provided with the GCS Service Account JSON, this will allow you to connect to the GCS Bucket in the Vectice App and get the needed data for the example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzVSDgQftE1p"
   },
   "source": [
    "## Credentials Setup:\n",
    "##### The Vectice API Endpoint and Token are needed to connect to the Vectice UI. Furthermore, a Google Cloud Storage credential JSON is needed to connect to the Google Cloud Storage to retrieve and upload the datasets. A project token links the runs to the relevant project and it's needed to create runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bls21M22rXVQ"
   },
   "outputs": [],
   "source": [
    "# Vectice API Endpoint\n",
    "os.environ['VECTICE_API_ENDPOINT'] ='https://beta.vectice.com'\n",
    "# The connection API token created in the Vectice Workspace\n",
    "os.environ['VECTICE_API_TOKEN'] = \"API_TOKEN\"\n",
    "# The Google Cloud Storage Service Account \n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"readerKey.json\"\n",
    "# Project token from Vectice UI\n",
    "PROJECT_TOKEN = \"TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we declare a reference to code existing in GitHub as the code at the origin of the outputs\n",
    "input_code = Vectice.create_code_version_with_github_uri(\"https://github.com/vectice/vectice-examples\",\n",
    "                        \"Notebooks/MLflow/Diamonds_Price_Prediction/Diamonds-Price-Predictions.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "2COUQbQ2rcId"
   },
   "outputs": [],
   "source": [
    "# Intialize the connection with Vectice\n",
    "vectice = Vectice(project_token=PROJECT_TOKEN)\n",
    "# Create a ds_version as an input for the run\n",
    "ds_version = vectice.create_dataset_version().with_parent_name(\"Diamonds\")\n",
    "# Create a run that will be passed into a start run\n",
    "run = vectice.create_run(\"Data Cleaning\", JobType.PREPARATION)\n",
    "# Start the run \n",
    "vectice.start_run(run, inputs = [input_code, ds_version])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpyiHjbGPvQs"
   },
   "source": [
    "This is an example how you would push your data into your GCS bucket. Throughout the tutorial you'll be interacting with GCS but we'll only be utilizing read rights. Thus, we won't be pushing any data into the GCS Bucket.\n",
    "\n",
    "```\n",
    "data.to_csv(\"gs://BUCKET/FILE_PATH/FILE_NAME.csv\", index = False, header = True)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUtOhVRlyvea"
   },
   "source": [
    "The dataset used in this tutorial can be retrieved from a Google Cloud Storage Bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLUvD7qO5dMm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"gs://vectice-examples-samples/Diamonds/diamonds.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ddBRGmDrnt8"
   },
   "outputs": [],
   "source": [
    "# This shows you the number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQISB5kEsp9u"
   },
   "outputs": [],
   "source": [
    "# The details of the data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV-C_cHYtzc7"
   },
   "source": [
    "### Data Cleaing \n",
    "In machine learning, if the data is irrelevant or error-prone then it leads to an incorrect model being built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-crp58S1stMt"
   },
   "outputs": [],
   "source": [
    "#Dropping dimentionless diamonds\n",
    "data = data.drop(data[data[\"x\"]==0].index)\n",
    "data = data.drop(data[data[\"y\"]==0].index)\n",
    "data = data.drop(data[data[\"z\"]==0].index)\n",
    "# We dropped 20 dimensionless entries\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgSNKWBosu3G"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data,hue= \"cut\", palette=\"rocket\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiZIdhMDs8JD"
   },
   "source": [
    "#### A few points to notice in these pair plots\n",
    "##### There are some features with datapoints that are far from the rest of the dataset which will affect the outcome of our regression model.\n",
    "\n",
    "* \"y\" and \"z\" have some dimensional outliers in our dataset that needs to be eliminated.\n",
    "* The \"depth\" should be capped but we must examine the regression line to be sure.\n",
    "* The \"table\" featured should be capped too.\n",
    "* Let's have a look at regression plots to get a close look at the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkZprTh3s9yt"
   },
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"price\", y=\"y\", data=data, scatter_kws={'color': 'purple'}, line_kws={'color': 'orange'}).set(title=\"Regression Line on Price vs 'y'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBhf4Dyzt5S3"
   },
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"price\", y=\"z\", data=data, scatter_kws={'color': 'purple'}, line_kws={'color': 'orange'}).set(title=\"Regression Line on Price vs 'z'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZkCLeU3t7Kd"
   },
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"price\", y=\"depth\", data=data, scatter_kws={'color': 'purple'}, line_kws={'color': 'orange'}).set(title=\"Regression Line on Price vs 'depth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLxpEpICt-23"
   },
   "source": [
    "We can clearly spot outliers in these attributes. Next up, we will remove these data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlubQwQ4t8vM"
   },
   "outputs": [],
   "source": [
    "#Dropping the outliers. \n",
    "data = data[(data[\"depth\"]<75)&(data[\"depth\"]>45)]\n",
    "data = data[(data[\"table\"]<80)&(data[\"table\"]>40)]\n",
    "data = data[(data[\"x\"]<30)]\n",
    "data = data[(data[\"y\"]<30)]\n",
    "data = data[(data[\"z\"]<30)&(data[\"z\"]>2)]\n",
    "# We dropped 13 outliers\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q-_4_fWuGWb"
   },
   "source": [
    "Let us have another look at the pair plot of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4l5pAtwkuFo1"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue= \"cut\",palette=\"rocket\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50kItC1buLXw"
   },
   "source": [
    "That's a much cleaner dataset. Next, we will deal with the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTokW8h6uJGc"
   },
   "outputs": [],
   "source": [
    "# Get list of categorical variables\n",
    "object_cols = [i for i in data.columns if data[i].dtype == 'object']\n",
    "print(f\"Categorical variables: {object_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfrAL_VIGkzA"
   },
   "source": [
    "#### Why are Categorical Features important?\n",
    "Machine learning models require all input and output variables to be numeric.\n",
    "\n",
    "This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97vBqZYAuVxv"
   },
   "source": [
    "#### We have three categorical variables. Let us have a look at them with violin plots.\n",
    "##### Violin plots are a method of plotting numeric data and can be considered a combination of the box plot with a kernel density plot. In the violin plot, we can find the same information as in the box plots:\n",
    "* median (a white dot on the violin plot)\n",
    "* interquartile range (the black bar in the center of violin)\n",
    "* the lower/upper adjacent values (the black lines stretched from the bar) — defined as first quartile — 1.5 IQR and third quartile + 1.5 IQR respectively. These values can be used in a simple outlier detection technique (Tukey’s fences) — observations lying outside of these “fences” can be considered outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cen5AfrixmEz"
   },
   "source": [
    "![Image](https://miro.medium.com/max/520/1*TTMOaNG1o4PgQd-e8LurMg.png)\n",
    "\n",
    "Probability Density Function:\n",
    "\n",
    "![Image](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Boxplot_vs_PDF.svg/525px-Boxplot_vs_PDF.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlWxM8r0ucfG"
   },
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x=\"cut\", y=\"price\", data=data).set(title=\"Violinplot for Cut vs Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nysEmMK3xq5g"
   },
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x=\"color\", y=\"price\", data=data).set(title=\"Violinplot for Color vs Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ditvhIpKxsiV"
   },
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x=\"clarity\", y=\"price\", data=data).set(title=\"Violinplot for Clarity vs Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRjqypZfxx9Q"
   },
   "source": [
    "#### Lable encoding the data to get rid of object dtype.\n",
    "This approach is very simple and it involves converting each value in a column to a number. Consider a dataset of bridges having a column names bridge-types having below values. Though there will be many more columns in the dataset, to understand label-encoding, we will focus on one categorical column only.We choose to encode the text values by putting a running sequence for each text values like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugf2Npxpx0bO"
   },
   "source": [
    "![Markdown Logo is here.](https://miro.medium.com/max/289/1*VinegxkUYMzik9GpucWCFA.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vwhlHUCxt02"
   },
   "outputs": [],
   "source": [
    "# Make copy to avoid changing original data \n",
    "label_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phme_1ZKxxy2"
   },
   "outputs": [],
   "source": [
    "def encoder_labels(columns: list, dataframe: pd.DataFrame, encoder: LabelEncoder) -> pd.DataFrame:\n",
    "    for col in columns:\n",
    "        dataframe[col] = encoder.fit_transform(dataframe[col])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Dm7fT6Nx413"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "label_data = encoder_labels(object_cols, label_data, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGBYeB0px6hu"
   },
   "outputs": [],
   "source": [
    "label_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kF0j123FyCKe"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk2dAM5PyFrn"
   },
   "source": [
    "#### Correlation Matrix:\n",
    "A correlation matrix is useful for showing the correlation coefficients (or degree of relationship) between variables. The correlation matrix is symmetric, as the correlation between a variable V1 and variable V2 is the same as the correlation between V2 and variable V1. Also, the values on the diagonal are always equal to one, because a variable is always perfectly correlated with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wR1WONgyDu_"
   },
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "corrmat= label_data.corr()\n",
    "f, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(corrmat,annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcAVhcvxyKN1"
   },
   "source": [
    "#### Points to notice:\n",
    "* \"x\", \"y\" and \"z\" show a high correlation to the target column.\n",
    "* \"depth\", \"cut\" and \"table\" show low correlation. We could consider dropping them but let's rather keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttaQqspIyICX"
   },
   "outputs": [],
   "source": [
    "# Create a new version of the orginal_cleaned dataset\n",
    "# We already uploaded the cleaned data to GCS. You can create a dataset using this data\n",
    "# An example of uploading the data to GCS -> label_data.to_csv(r'gs://\"GCS_URI\", index = False, header = True)\n",
    "outputs = [vectice.create_dataset_version().with_parent_name(\"Diamonds Cleaned\")]\n",
    "# End the run and save the new dataset version.\n",
    "# Set the diamonds cleaned as an output.\n",
    "vectice.end_run(outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "t0Ij6-DRyW6t"
   },
   "outputs": [],
   "source": [
    "# Create inputs \n",
    "ds_version = vectice.create_dataset_version().with_parent_name(\"Diamonds Cleaned\")\n",
    "# Start a run to track this data train-test-split\n",
    "# It will specify the dataset version we just created as the run's input.\n",
    "run = vectice.create_run('Split Diamonds Data', JobType.PREPARATION)\n",
    "# Start the run\n",
    "vectice.start_run(run, inputs=[input_code, ds_version])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eni9eg4PybdN"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(label_data, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Qm5wznCyd7l"
   },
   "outputs": [],
   "source": [
    "# The key you were provided for this tutorial may not have write permissions to GCS.\n",
    "# Example of uploading to the GCS bucket -> train.to_csv (r'GCS_URI', index = False, header = True)\n",
    "# Example of uploading to the GCS bucket -> test.to_csv (r'GCS_URI', index = False, header = True)\n",
    "# We already uploaded the data to GCS. You can create a dataset using this data and add it as the output of this run\n",
    "outputs = [vectice.create_dataset_version().with_parent_name(\"Diamonds Train Test Split\")]\n",
    "# End the run\n",
    "vectice.end_run(outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO3M1I0zykfH"
   },
   "source": [
    "### Model Building\n",
    "#### Steps involved in Model Building\n",
    "\n",
    "* Setting up features and target\n",
    "* Build a pipeline of standard scalar and model for five different regressors.\n",
    "* Fit all the models on training data\n",
    "* Get mean of cross-validation on the training set for all the models for negative root mean square error\n",
    "* Pick the model with the best cross-validation score\n",
    "* Fit the best model on the training set and predict on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzGujRX6ynMB"
   },
   "source": [
    "### Train-Test Split Evaluation \n",
    "The procedure involves taking a dataset and dividing it into two subsets. The first subset is used to fit the model and is referred to as the training dataset. The second subset is not used to train the model; instead, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values. This second dataset is referred to as the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDDmkPYWylKy"
   },
   "outputs": [],
   "source": [
    "# Assigning the featurs as X and trarget as y\n",
    "X = label_data.drop([\"price\"], axis =1)\n",
    "y = label_data[\"price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "QiAhpKX2APeQ"
   },
   "outputs": [],
   "source": [
    "# Initialise Vectice with MLflow\n",
    "vectice = Vectice(project_token=PROJECT_TOKEN, tracking_uri=\"https://mlflow-beta.vectice.com\",lib=\"MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWDbwJ3eyv2x"
   },
   "outputs": [],
   "source": [
    "# Create the inputs\n",
    "def create_inputs():\n",
    "    return [\n",
    "        Vectice.create_dataset_version().with_parent_name(\"Diamonds Train Test Split\"),\n",
    "        input_code\n",
    "    ]\n",
    "# Data preparation\n",
    "def prepare_data():\n",
    "    \"\"\"Read and prepare data.\"\"\"\n",
    "    df = pd.read_csv(r\"gs://vectice-examples-samples/Diamonds/diamonds_cleaned.csv\")\n",
    "\n",
    "    X = df.drop([\"price\"], axis =1)\n",
    "    y = df[\"price\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NqzDN41y0Ff"
   },
   "source": [
    "### Pipelines\n",
    "In most machine learning projects the data that you have to work with is unlikely to be in the ideal format for producing the best performing model. There are quite often a number of transformational steps such as encoding categorical variables, feature scaling and normalisation that need to be performed. Scikit-learn has built in functions for most of these commonly used transformations in it’s preprocessing package.\n",
    "However, in a typical machine learning workflow you will need to apply all these transformations at least twice. Once when training the model and again on any new data you want to predict on. Of course you could write a function to apply them and reuse that but you would still need to run this first and then call the model separately. Scikit-learn pipelines are a tool to simplify this process. They have several key benefits:\n",
    "* They make your workflow much easier to read and understand.\n",
    "* They enforce the implementation and order of steps in your project.\n",
    "* These in turn make your work much more reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me1SdV6My47Y"
   },
   "source": [
    "### StandardScaler Example:\n",
    "A StandardScaler substarcts the mean and then divides by the standard deviation, this shifts the distribution to have a mean of 0 and a standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlKQwjaMy4On"
   },
   "outputs": [],
   "source": [
    "example = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "scaler = StandardScaler().fit(example)\n",
    "X_scaled = scaler.transform(example)\n",
    "print(f\"Before: {example[0]} \\nAfter: {X_scaled[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr2c54gvy-4k"
   },
   "source": [
    "### Cross Validation:\n",
    "\n",
    "Cross validation follows the following logic. A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "- A model is trained using of the folds as training data;\n",
    "\n",
    "- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "![Image](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hxu31e5MzB3p"
   },
   "source": [
    "### Models:\n",
    "\n",
    "1. LinearRegression <a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html\" target=\"_blank\">more info</a>.\n",
    "2. DecisionTreeRegressor <a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#decision-trees\" target=\"_blank\">more info</a>.\n",
    "3. RandomForestRegressor <a href=\"https://www.geeksforgeeks.org/random-forest-regression-in-python/\" target=\"_blank\">more info</a>.\n",
    "4. KNeighborsRegressor <a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#k-nearest-neighbor\" target=\"_blank\">more info</a>.\n",
    "5. XGBRegressor <a href=\"https://machinelearningmastery.com/xgboost-for-regression/\" target=\"_blank\">more info</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkQ2WUpZy7z3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\"\"\"Vectice MLflow adapter fluent usage in Python ``with`` syntax.\"\"\"\n",
    "X_train, X_test, y_train, y_test = prepare_data()\n",
    "\n",
    "mlflow.autolog(silent=True)\n",
    "\n",
    "\n",
    "# Set up the MLflow tracking uri\n",
    "mlflow.set_tracking_uri(\"https://mlflow-beta.vectice.com\")\n",
    "\n",
    "# Building pipelins of standard scaler and model for regressors.\n",
    "pipeline_lr=Pipeline([(\"scalar1\",StandardScaler()),\n",
    "                 (\"Diamonds_Regressor\",LinearRegression())])\n",
    "\n",
    "pipeline_dt=Pipeline([(\"scalar2\",StandardScaler()),\n",
    "                    (\"Diamonds_Regressor\",DecisionTreeRegressor())])\n",
    "\n",
    "pipeline_rf=Pipeline([(\"scalar3\",StandardScaler()),\n",
    "                    (\"Diamonds_Regressor\",RandomForestRegressor())])\n",
    "\n",
    "\n",
    "pipeline_kn=Pipeline([(\"scalar4\",StandardScaler()),\n",
    "                    (\"Diamonds_Regressor\",KNeighborsRegressor())])\n",
    "\n",
    "\n",
    "pipeline_xgb=Pipeline([(\"scalar5\",StandardScaler()),\n",
    "                    (\"Diamonds_Regressor\",XGBRegressor())])\n",
    "\n",
    "# Pipelines list to iterate over\n",
    "pipelines = [pipeline_lr, pipeline_dt, pipeline_rf, pipeline_kn, pipeline_xgb]\n",
    "\n",
    "for pipe in pipelines:\n",
    "    # Create inputs for each Vectice & MLflow run\n",
    "    inputs = create_inputs()\n",
    "    # Expermient name for each pipeline \n",
    "    MLFLOW_EXPERIMENT_NAME = pipe.steps[1][0]\n",
    "    mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "    algorithm = pipe.steps[1][1]\n",
    "    \n",
    "    # Fit each model \n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        cv_score = cross_val_score(pipe, X_train, y_train,scoring=\"neg_root_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "        mlflow.log_param('Algorithm', algorithm)\n",
    "        mlflow.log_param('Scaler', 'StandardScaler')\n",
    "        mlflow.log_metric(\"Cross Validation\", float(cv_score.mean()))\n",
    "        print(f\"{MLFLOW_EXPERIMENT_NAME}: {cv_score.mean()}\")\n",
    "        run_data = mlflow.get_run(run_id=run.info.run_id)\n",
    "  \n",
    "    Vectice.save_after_run(project_token=PROJECT_TOKEN, run=run_data, lib=\"MLflow\", inputs = create_inputs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can have the list of models in the project by using vectice.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectice.list_models().list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update an existing model by using vectice.update_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectice.update_model(parent_name=\"Diamonds_Regressor\", model_type=ModelType.REGRESSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HMKpZxAzNhl"
   },
   "source": [
    "#### Testing the Model with the best score on the test set\n",
    "In the above scores, Random Forest appears to be the model with the best scoring on negative root mean square error. Let's test this model on a test set and evaluate it with different parameters. But you might get different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVBT_UQc5dM_"
   },
   "outputs": [],
   "source": [
    "# If you have a mlflow run that is still running and you need to end it, then run this cell.\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcwMnsWhzE2J"
   },
   "outputs": [],
   "source": [
    "# Model prediction on test data\n",
    "pred = pipeline_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xjzp8oIAz666"
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "print(\"R^2:\",metrics.r2_score(y_test, pred))\n",
    "print(\"Adjusted R^2:\",1 - (1-metrics.r2_score(y_test, pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n",
    "print(\"MAE:\",metrics.mean_absolute_error(y_test, pred))\n",
    "print(\"MSE:\",metrics.mean_squared_error(y_test, pred))\n",
    "print(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md9YwqEGz_SZ"
   },
   "source": [
    "#### End\n",
    "\n",
    "Congratulations and as Jake Peralta would say:\n",
    "\n",
    "![Image](https://i.imgur.com/I1wR7mE.gif?noredirect)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Diamonds-Price-Predictions.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "5970002f2814f9b5c80d9d4fc10f2bf2fda215a6ce99a901ef026501edeb9abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
