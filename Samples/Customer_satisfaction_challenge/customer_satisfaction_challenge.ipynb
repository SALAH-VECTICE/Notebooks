{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "view-in-github",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vectice/vectice-examples/blob/master/Samples/Customer_satisfaction_challenge/customer_satisfaction_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c65d22b9",
      "metadata": {
        "id": "c65d22b9"
      },
      "source": [
        "# Santander customer satisfaction challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd618113",
      "metadata": {
        "id": "cd618113"
      },
      "source": [
        "## Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c53800",
      "metadata": {
        "id": "13c53800"
      },
      "source": [
        "Customer satisfaction is a key measure of success for all businesses. Unhappy customers don't stay with the same provider and they rarely voice their dissatisfaction before leaving. In this context, Santander bank launched a challenge in Kaggle in order to build models that predict potential unhappy customers\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb29db7c",
      "metadata": {
        "id": "eb29db7c"
      },
      "source": [
        "## Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba76d94d",
      "metadata": {
        "id": "ba76d94d"
      },
      "source": [
        "The objective of this competition is to be able to identify unhappy customers early and anticipate their leaving which would allow the company to take proactive steps to improve a customer's happiness before it's too late. In this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a9a7e8",
      "metadata": {
        "id": "26a9a7e8"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a0c7084",
      "metadata": {
        "id": "6a0c7084"
      },
      "source": [
        "The data is an anonymized dataset containing a large number of numeric variables. The \"TARGET\" column is the variable to predict. It equals 1 for unsatisfied customers and 0 for satisfied customers. The task is to predict the probability that each customer in the test set is an unsatisfied customer.\n",
        "- train.csv: (371 columns): The training set including the target\n",
        "- test.csv: (370 columns): The test set without the target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fc51dd4",
      "metadata": {
        "id": "4fc51dd4"
      },
      "source": [
        "## Install Vectice and GCS packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c3ccd71",
      "metadata": {
        "id": "4c3ccd71"
      },
      "source": [
        "Vectice provides a generic metadata layer that is potentially suitable for most data science workflows. For this notebook we will use the sickit-learn library for modeling and track experiments directly through our Python SDK to illustrate how to fine-tune exactly what you would like to track: metrics, etc. The same mechanisms would apply to R, Java or even more generic REST APIs to track metadata from any programming language and library.\n",
        "\n",
        "Here is a link to the [Vectice Python library documentation](https://doc.vectice.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02e4135",
      "metadata": {
        "id": "a02e4135"
      },
      "outputs": [],
      "source": [
        "## Install GCS packages\n",
        "!pip install --q fsspec\n",
        "!pip install --q gcsfs\n",
        "\n",
        "#Install Vectice Python library \n",
        "# In this notebook we will do code versioning using github, we also support gitlab\n",
        "# and bitbucket: !pip install -q \"vectice[github, gitlab, bitbucket]\"\n",
        "!pip install --q vectice[github]==2.2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9d12a9",
      "metadata": {
        "id": "7d9d12a9"
      },
      "outputs": [],
      "source": [
        "!pip3 show vectice"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9b465b6",
      "metadata": {
        "id": "d9b465b6"
      },
      "source": [
        "## Import the required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2bc921",
      "metadata": {
        "id": "0e2bc921"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, auc\n",
        "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
        "import lightgbm as lgb\n",
        "from lightgbm import plot_importance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e63a557",
      "metadata": {
        "id": "7e63a557"
      },
      "source": [
        "## Retreive the data from GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-JyVpzG_dM-H",
      "metadata": {
        "id": "-JyVpzG_dM-H"
      },
      "source": [
        "We are going to load data stored in Google Cloud Storage, that is provided by Vectice for this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9v2HQ-oI9BWu",
      "metadata": {
        "id": "9v2HQ-oI9BWu"
      },
      "outputs": [],
      "source": [
        "# Download the \"JSON file\" from the \"Vectice tutorial Page\" in the application so that \n",
        "# you can access the GCS bucket. The name of the JSON file should be \"readerKey.json\"\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b53c2b1",
      "metadata": {
        "id": "1b53c2b1"
      },
      "outputs": [],
      "source": [
        "# Double check the json file name below so that it matches the name of the file that you uploaded.\n",
        "# Note that the key provided for this notebook does not have permissions for you to write to GCS. \n",
        "# You can only use it to read the data.\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'readerKey.json'\n",
        "## Get the dataset from GCS\n",
        "train_df = pd.read_csv(\"gs://vectice-examples-samples/Customer_satisfaction_challenge/dataset.csv\")\n",
        "# Run head to make sure the data was loaded properly\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e786cc41",
      "metadata": {
        "id": "e786cc41"
      },
      "source": [
        "## Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92aff10e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:08.505604Z",
          "iopub.status.busy": "2021-04-10T09:30:08.504426Z",
          "iopub.status.idle": "2021-04-10T09:30:08.507867Z",
          "shell.execute_reply": "2021-04-10T09:30:08.506112Z"
        },
        "id": "92aff10e",
        "papermill": {
          "duration": 0.04008,
          "end_time": "2021-04-10T09:30:08.507985",
          "exception": false,
          "start_time": "2021-04-10T09:30:08.467905",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"Train Data Shape : \",train_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5bebe78",
      "metadata": {
        "id": "b5bebe78"
      },
      "outputs": [],
      "source": [
        "train_df['TARGET'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f82dd09e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:08.595439Z",
          "iopub.status.busy": "2021-04-10T09:30:08.594699Z",
          "iopub.status.idle": "2021-04-10T09:30:08.599672Z",
          "shell.execute_reply": "2021-04-10T09:30:08.599131Z"
        },
        "id": "f82dd09e",
        "papermill": {
          "duration": 0.061924,
          "end_time": "2021-04-10T09:30:08.599807",
          "exception": false,
          "start_time": "2021-04-10T09:30:08.537883",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "913fa320",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:08.740556Z",
          "iopub.status.busy": "2021-04-10T09:30:08.739623Z",
          "iopub.status.idle": "2021-04-10T09:30:10.278376Z",
          "shell.execute_reply": "2021-04-10T09:30:10.278859Z"
        },
        "id": "913fa320",
        "papermill": {
          "duration": 1.646631,
          "end_time": "2021-04-10T09:30:10.279041",
          "exception": false,
          "start_time": "2021-04-10T09:30:08.632410",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f1ad5c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:10.424731Z",
          "iopub.status.busy": "2021-04-10T09:30:10.422820Z",
          "iopub.status.idle": "2021-04-10T09:30:10.425415Z",
          "shell.execute_reply": "2021-04-10T09:30:10.425819Z"
        },
        "id": "f5f1ad5c",
        "papermill": {
          "duration": 0.113231,
          "end_time": "2021-04-10T09:30:10.426014",
          "exception": false,
          "start_time": "2021-04-10T09:30:10.312783",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "features = train_df.drop(['ID','TARGET'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8c3198",
      "metadata": {
        "id": "cc8c3198",
        "papermill": {
          "duration": 0.033393,
          "end_time": "2021-04-10T09:30:10.493222",
          "exception": false,
          "start_time": "2021-04-10T09:30:10.459829",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Exploratory data analysis (EDA)\n",
        "* Target Percent\n",
        "* Check Multicollinearity\n",
        "* Check Outlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d9dde2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:10.564726Z",
          "iopub.status.busy": "2021-04-10T09:30:10.563609Z",
          "iopub.status.idle": "2021-04-10T09:30:10.574704Z",
          "shell.execute_reply": "2021-04-10T09:30:10.574163Z"
        },
        "id": "02d9dde2",
        "papermill": {
          "duration": 0.048701,
          "end_time": "2021-04-10T09:30:10.574875",
          "exception": false,
          "start_time": "2021-04-10T09:30:10.526174",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(train_df['TARGET'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f968764e",
      "metadata": {
        "id": "f968764e"
      },
      "source": [
        "The training set is way imbalanced (73012 zeros vs 3008 ones), so some algorithms may learn mostly from the 0 which can affect our predictions. We address that by using oversampling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c79ec39",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:10.661950Z",
          "iopub.status.busy": "2021-04-10T09:30:10.650205Z",
          "iopub.status.idle": "2021-04-10T09:30:10.960179Z",
          "shell.execute_reply": "2021-04-10T09:30:10.959645Z"
        },
        "id": "9c79ec39",
        "papermill": {
          "duration": 0.351011,
          "end_time": "2021-04-10T09:30:10.960347",
          "exception": false,
          "start_time": "2021-04-10T09:30:10.609336",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(1,2,figsize=(10,4))\n",
        "train_df['TARGET'].value_counts().plot.pie(\n",
        "    explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True\n",
        ")\n",
        "sns.countplot('TARGET', data=train_df, ax=ax[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ef4f58",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:11.038954Z",
          "iopub.status.busy": "2021-04-10T09:30:11.037745Z",
          "iopub.status.idle": "2021-04-10T09:30:11.169433Z",
          "shell.execute_reply": "2021-04-10T09:30:11.169937Z"
        },
        "id": "28ef4f58",
        "papermill": {
          "duration": 0.17417,
          "end_time": "2021-04-10T09:30:11.170170",
          "exception": false,
          "start_time": "2021-04-10T09:30:10.996000",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "null_value = train_df.isnull().sum().sort_values(ascending=False)\n",
        "null_percent = round(train_df.isnull().sum().sort_values(ascending=False)/len(train_df)*100,2)\n",
        "pd.concat([null_value, null_percent], axis=1, keys=['Null values', 'Percent'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6618c7cd",
      "metadata": {
        "id": "6618c7cd"
      },
      "source": [
        "Ther is no column with null values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcde4213",
      "metadata": {
        "id": "fcde4213"
      },
      "source": [
        "**Correlation**\n",
        "\n",
        "If we have a big correlation, we have a problem of multicolinearity. That means that there are some features that depend of other features, so we should reduce the dimentionality of our data (if A depends of B, we should either find a way to aggregate or combine the two features and turn it into one variable or drop one of the variables that are too highly correlated with another) and that can be adressed using Principal component analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50606c02",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:11.257537Z",
          "iopub.status.busy": "2021-04-10T09:30:11.256841Z",
          "iopub.status.idle": "2021-04-10T09:30:11.287243Z",
          "shell.execute_reply": "2021-04-10T09:30:11.287732Z"
        },
        "id": "50606c02",
        "papermill": {
          "duration": 0.07645,
          "end_time": "2021-04-10T09:30:11.287892",
          "exception": false,
          "start_time": "2021-04-10T09:30:11.211442",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "features[features.columns[:8]].corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0547ae73",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:11.374340Z",
          "iopub.status.busy": "2021-04-10T09:30:11.373115Z",
          "iopub.status.idle": "2021-04-10T09:30:12.063179Z",
          "shell.execute_reply": "2021-04-10T09:30:12.063689Z"
        },
        "id": "0547ae73",
        "papermill": {
          "duration": 0.736357,
          "end_time": "2021-04-10T09:30:12.063872",
          "exception": false,
          "start_time": "2021-04-10T09:30:11.327515",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "sns.heatmap(features[features.columns[:8]].corr(),annot=True,cmap='YlGnBu')\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(10,8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9917207",
      "metadata": {
        "id": "e9917207",
        "papermill": {
          "duration": 0.036253,
          "end_time": "2021-04-10T09:30:12.138938",
          "exception": false,
          "start_time": "2021-04-10T09:30:12.102685",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "=> We Can Check Multicollinearity\n",
        "\n",
        "Multicollinearity is a phenomenon in which one independent variable is highly correlated with one or more of the other independent variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7727ec3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:12.234856Z",
          "iopub.status.busy": "2021-04-10T09:30:12.228916Z",
          "iopub.status.idle": "2021-04-10T09:30:14.659466Z",
          "shell.execute_reply": "2021-04-10T09:30:14.658480Z"
        },
        "id": "a7727ec3",
        "papermill": {
          "duration": 2.483,
          "end_time": "2021-04-10T09:30:14.659617",
          "exception": false,
          "start_time": "2021-04-10T09:30:12.176617",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,6))\n",
        "plt.title(\"Distribution of mean values per row in the train and test set\")\n",
        "sns.distplot(train_df[features.columns].mean(axis=1),color=\"black\", kde=True,bins=120, label='train')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d5e6a03",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:14.762204Z",
          "iopub.status.busy": "2021-04-10T09:30:14.759793Z",
          "iopub.status.idle": "2021-04-10T09:30:17.006201Z",
          "shell.execute_reply": "2021-04-10T09:30:17.006691Z"
        },
        "id": "8d5e6a03",
        "papermill": {
          "duration": 2.308513,
          "end_time": "2021-04-10T09:30:17.006870",
          "exception": false,
          "start_time": "2021-04-10T09:30:14.698357",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,6))\n",
        "plt.title(\"Distribution of std values per rows in the train and test set\")\n",
        "sns.distplot(train_df[features.columns].std(axis=1),color=\"blue\",kde=True,bins=120, label='train')\n",
        "plt.legend(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a75dc43a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:17.098785Z",
          "iopub.status.busy": "2021-04-10T09:30:17.097753Z",
          "iopub.status.idle": "2021-04-10T09:30:19.059778Z",
          "shell.execute_reply": "2021-04-10T09:30:19.059210Z"
        },
        "id": "a75dc43a",
        "papermill": {
          "duration": 2.010971,
          "end_time": "2021-04-10T09:30:19.059925",
          "exception": false,
          "start_time": "2021-04-10T09:30:17.048954",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "t0 = train_df[train_df['TARGET'] == 0]\n",
        "t1 = train_df[train_df['TARGET'] == 1]\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.title(\"Distribution of skew values per row in the train set\")\n",
        "sns.distplot(t0[features.columns].skew(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\n",
        "sns.distplot(t1[features.columns].skew(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\n",
        "plt.legend(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc2d0ca7",
      "metadata": {
        "id": "fc2d0ca7",
        "papermill": {
          "duration": 0.038339,
          "end_time": "2021-04-10T09:30:19.138352",
          "exception": false,
          "start_time": "2021-04-10T09:30:19.100013",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "=> We Can Check Outliers\n",
        "\n",
        "An outlier is a value or point that differs substantially from the rest of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fcaa813",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:19.292152Z",
          "iopub.status.busy": "2021-04-10T09:30:19.291019Z",
          "iopub.status.idle": "2021-04-10T09:30:20.736357Z",
          "shell.execute_reply": "2021-04-10T09:30:20.735793Z"
        },
        "id": "6fcaa813",
        "papermill": {
          "duration": 1.558729,
          "end_time": "2021-04-10T09:30:20.736523",
          "exception": false,
          "start_time": "2021-04-10T09:30:19.177794",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b92157",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:20.846567Z",
          "iopub.status.busy": "2021-04-10T09:30:20.845574Z",
          "iopub.status.idle": "2021-04-10T09:30:20.947011Z",
          "shell.execute_reply": "2021-04-10T09:30:20.947475Z"
        },
        "id": "f3b92157",
        "papermill": {
          "duration": 0.166387,
          "end_time": "2021-04-10T09:30:20.947629",
          "exception": false,
          "start_time": "2021-04-10T09:30:20.781242",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.boxplot(train_df['var3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1813ccb0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:21.037116Z",
          "iopub.status.busy": "2021-04-10T09:30:21.036179Z",
          "iopub.status.idle": "2021-04-10T09:30:21.150551Z",
          "shell.execute_reply": "2021-04-10T09:30:21.150989Z"
        },
        "id": "1813ccb0",
        "papermill": {
          "duration": 0.161176,
          "end_time": "2021-04-10T09:30:21.151170",
          "exception": false,
          "start_time": "2021-04-10T09:30:20.989994",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.boxplot(train_df['var38'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15d5668a",
      "metadata": {
        "id": "15d5668a"
      },
      "source": [
        "The training set:\n",
        "- Contains continuous and and catigorized data (we should treate carigorized data cuz 10000>1 if we interpret them as numeric values and not catigorical (example IDs)\n",
        "- Contains variables with zero variance or non predictive value\n",
        "- Contains fake values (-999999) that were introduced to replace missing data\n",
        "- Is way imbalanced"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8637fb5",
      "metadata": {
        "id": "f8637fb5",
        "papermill": {
          "duration": 0.043992,
          "end_time": "2021-04-10T09:30:21.238920",
          "exception": false,
          "start_time": "2021-04-10T09:30:21.194928",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "* Processing Outlier Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "075a5caa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:21.329714Z",
          "iopub.status.busy": "2021-04-10T09:30:21.328663Z",
          "iopub.status.idle": "2021-04-10T09:30:22.622258Z",
          "shell.execute_reply": "2021-04-10T09:30:22.622990Z"
        },
        "id": "075a5caa",
        "papermill": {
          "duration": 1.341621,
          "end_time": "2021-04-10T09:30:22.623220",
          "exception": false,
          "start_time": "2021-04-10T09:30:21.281599",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_df['var3'].replace(-999999,2,inplace=True)\n",
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "434d729b",
      "metadata": {
        "id": "434d729b"
      },
      "source": [
        "## Vectice Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gHqZV6Ja54tH",
      "metadata": {
        "id": "gHqZV6Ja54tH"
      },
      "outputs": [],
      "source": [
        "from vectice import Experiment\n",
        "from vectice.api.json import ModelType\n",
        "from vectice.api.json import JobType\n",
        "\n",
        "# Specify the API endpoint for Vectice.\n",
        "# You can specify your API endpoint here in the notebook, but we recommand you to add it to a .env file\n",
        "os.environ['VECTICE_API_ENDPOINT']= \"app.vectice.com\"\n",
        "\n",
        "# To use the Vectice Python library, you first need to authenticate your account using an API key.\n",
        "# You can generate an API key from the Vectice UI, by going to the \"My API Keys\" section under your profile's picture\n",
        "# You can specify your API Token here in the notebook, but we recommand you to add it to a .env file\n",
        "os.environ['VECTICE_API_TOKEN'] = \"Your API Token\"\n",
        "\n",
        "# Add you project id. The project id can be found in the project settings page in the Vectice UI\n",
        "PROJECT_ID = ID"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e341732",
      "metadata": {
        "id": "3e341732",
        "papermill": {
          "duration": 0.043784,
          "end_time": "2021-04-10T09:30:22.714674",
          "exception": false,
          "start_time": "2021-04-10T09:30:22.670890",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "In this part we will:\n",
        "* Split Data to Train / Test \n",
        "* Train Data to Standard Scaler\n",
        "* Target Data to Oversampling by SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bCpyyNIMSahh",
      "metadata": {
        "id": "bCpyyNIMSahh"
      },
      "outputs": [],
      "source": [
        "train_df.drop('ID',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "901292cd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:23.107800Z",
          "iopub.status.busy": "2021-04-10T09:30:23.086344Z",
          "iopub.status.idle": "2021-04-10T09:30:23.111031Z",
          "shell.execute_reply": "2021-04-10T09:30:23.110577Z"
        },
        "id": "901292cd",
        "papermill": {
          "duration": 0.12009,
          "end_time": "2021-04-10T09:30:23.111160",
          "exception": false,
          "start_time": "2021-04-10T09:30:22.991070",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "x = train_df.drop('TARGET',axis=1)\n",
        "y = train_df['TARGET']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0672d3",
      "metadata": {
        "id": "bc0672d3"
      },
      "source": [
        "### Resolving the problem of multicolinearity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8083d72",
      "metadata": {
        "id": "c8083d72"
      },
      "source": [
        "Here we are going to use the \"The Pearson correlation\" method. It is the most common method to use for numerical variables; it assigns a value between − 1 and 1, where 0 is no correlation, 1 is total positive correlation, and − 1 is total negative correlation. This is interpreted as follows: a correlation value of 0.7 between two variables would indicate that a significant and positive relationship exists between the two. A positive correlation signifies that if variable A goes up, then B will also go up, whereas if the value of the correlation is negative, then if A increases, B decreases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a7505c",
      "metadata": {
        "id": "13a7505c"
      },
      "outputs": [],
      "source": [
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()  # Set of all the names of correlated columns\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
        "                colname = corr_matrix.columns[i]  # getting the name of column\n",
        "                col_corr.add(colname)\n",
        "    return col_corr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8795276c",
      "metadata": {
        "id": "8795276c"
      },
      "source": [
        "We consider a threshold of 0.9 to avoid high correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc7f0c8",
      "metadata": {
        "id": "7fc7f0c8"
      },
      "outputs": [],
      "source": [
        "corr_features = correlation(x, 0.9)\n",
        "len(set(corr_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9a9fd0",
      "metadata": {
        "id": "ae9a9fd0"
      },
      "outputs": [],
      "source": [
        "x = x.drop(corr_features,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bde1a4b8",
      "metadata": {
        "id": "bde1a4b8"
      },
      "source": [
        "### Standardize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3352937",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:23.209151Z",
          "iopub.status.busy": "2021-04-10T09:30:23.207792Z",
          "iopub.status.idle": "2021-04-10T09:30:24.155479Z",
          "shell.execute_reply": "2021-04-10T09:30:24.154951Z"
        },
        "id": "f3352937",
        "papermill": {
          "duration": 1.001235,
          "end_time": "2021-04-10T09:30:24.155631",
          "exception": false,
          "start_time": "2021-04-10T09:30:23.154396",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler().fit(x)\n",
        "x_scaler = scaler.transform(x)\n",
        "x_scaler_df = pd.DataFrame(x_scaler, columns=x.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1858b9f4",
      "metadata": {
        "id": "1858b9f4"
      },
      "source": [
        "**Principal component analysis (PCA)** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f55578",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:24.246867Z",
          "iopub.status.busy": "2021-04-10T09:30:24.245829Z",
          "iopub.status.idle": "2021-04-10T09:30:28.726482Z",
          "shell.execute_reply": "2021-04-10T09:30:28.725973Z"
        },
        "id": "58f55578",
        "papermill": {
          "duration": 4.528332,
          "end_time": "2021-04-10T09:30:28.726624",
          "exception": false,
          "start_time": "2021-04-10T09:30:24.198292",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=0.95)\n",
        "x_scaler_pca = pca.fit_transform(x_scaler)\n",
        "x_scaler_pca_df = pd.DataFrame(x_scaler_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc6033c6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:28.837090Z",
          "iopub.status.busy": "2021-04-10T09:30:28.836300Z",
          "iopub.status.idle": "2021-04-10T09:30:28.840095Z",
          "shell.execute_reply": "2021-04-10T09:30:28.840581Z"
        },
        "id": "dc6033c6",
        "papermill": {
          "duration": 0.070204,
          "end_time": "2021-04-10T09:30:28.840741",
          "exception": false,
          "start_time": "2021-04-10T09:30:28.770537",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "x_scaler_pca_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "527ca72d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:28.937349Z",
          "iopub.status.busy": "2021-04-10T09:30:28.936342Z",
          "iopub.status.idle": "2021-04-10T09:30:28.940300Z",
          "shell.execute_reply": "2021-04-10T09:30:28.940805Z"
        },
        "id": "527ca72d",
        "papermill": {
          "duration": 0.053612,
          "end_time": "2021-04-10T09:30:28.940973",
          "exception": false,
          "start_time": "2021-04-10T09:30:28.887361",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b042a6d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:29.052183Z",
          "iopub.status.busy": "2021-04-10T09:30:29.051277Z",
          "iopub.status.idle": "2021-04-10T09:30:30.526530Z",
          "shell.execute_reply": "2021-04-10T09:30:30.526986Z"
        },
        "id": "8b042a6d",
        "papermill": {
          "duration": 1.540039,
          "end_time": "2021-04-10T09:30:30.527130",
          "exception": false,
          "start_time": "2021-04-10T09:30:28.987091",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.scatter(x_scaler_pca_df.loc[:, 0], x_scaler_pca_df.loc[:, 1], c=y,  cmap=\"copper_r\")\n",
        "plt.axis('off')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d171e71a",
      "metadata": {
        "id": "d171e71a",
        "papermill": {
          "duration": 0.045353,
          "end_time": "2021-04-10T09:30:30.619214",
          "exception": false,
          "start_time": "2021-04-10T09:30:30.573861",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "=> We cant use PCA since we can't reduce the dimentionality (The variance is represented by multiple variables and we didn't find a small number of variables that enable to represent a considerable part of the variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-3PKP2Ux8bbd",
      "metadata": {
        "id": "-3PKP2Ux8bbd"
      },
      "source": [
        "## Split the data and use oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJo22Fp-SY6b",
      "metadata": {
        "id": "mJo22Fp-SY6b"
      },
      "outputs": [],
      "source": [
        "# We create our first experiment for data preparation and specify the workspace and the project we will be working on\n",
        "# Each experiment only contains one job. Each invokation of the job is called a run.\n",
        "# autocode = True enables you to track your git changes for your code automatically every time you execute a run (see below).\n",
        "experiment = Experiment(job=\"jobSplitData_Customer_Satisfaction\", job_type = JobType.PREPARATION, project=PROJECT_ID, auto_code = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h1VNt9cLQuWw",
      "metadata": {
        "id": "h1VNt9cLQuWw"
      },
      "source": [
        "We can check if the datasets are already created in our workspace by calling **experiment.vectice.list_datasets()** which lists all the datasets existing in the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uroyfyZ9QuWw",
      "metadata": {
        "id": "uroyfyZ9QuWw"
      },
      "outputs": [],
      "source": [
        "experiment.vectice.list_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4Zeh0JLQuWw",
      "metadata": {
        "id": "d4Zeh0JLQuWw"
      },
      "source": [
        "Create a dataset version based on the created/existing dataset that contains your data. For this notebook, we'll use some datasets that have already been created in Vectice to illustrate datasets auto-versioning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GRHuo2f92kcx",
      "metadata": {
        "id": "GRHuo2f92kcx"
      },
      "source": [
        "The following code splits the dataset to train and test sets and uses the SMOTE methode for oversampling in order to balance our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y0NsPUJR209T",
      "metadata": {
        "id": "y0NsPUJR209T"
      },
      "outputs": [],
      "source": [
        "# We use auto-versioning here.\n",
        "# The Vectice library automatically detects if there have been changes to the dataset you are using.\n",
        "# If it detects changes, it will generate a new version of your dataset automatically.\n",
        "# For this notebook, we changed the data to illustrate datasets auto-versioning..\n",
        "# So, the Vectice Python library will create a new dataset version when this code is executed for the first time.\n",
        "input_ds_version = experiment.add_dataset_version(dataset=\"customer_satisfaction_dataset\")\n",
        "\n",
        "# Because we are using Colab in this tutorial example we are going to declare a reference to the code\n",
        "## manually. This will be added as a reference to the run we are going to create next.\n",
        "# If you are using your local environment with GIT installed or JupyterLab etc... the code\n",
        "# tracking is automated.\n",
        "uri = \"https://github.com/vectice/vectice-examples\"\n",
        "entrypoint=\"Samples/Customer_satisfaction_challenge/customer_satisfaction_challenge.ipynb\"\n",
        "input_code = experiment.add_code_version_uri(git_uri=uri, entrypoint=entrypoint)\n",
        "\n",
        "# The created dataset version and code version will be automatically attached as inputs of the run as they come before the experiment.start\n",
        "experiment.start(run_properties={\"Property1\": \"Value 1\", \"property2\": \"Value 2\"})\n",
        "\n",
        "#Split data\n",
        "scaler_x_train, scaler_x_test, scaler_y_train, scaler_y_test = train_test_split(x_scaler, y, test_size=0.3)\n",
        "#Use SMOTE to oversample the dataset\n",
        "x_over, y_over = SMOTE().fit_resample(scaler_x_train,scaler_y_train)\n",
        "print(sorted(Counter(y_over).items()))\n",
        "\n",
        "\n",
        "# We commented out the code to persist the training and testing test in GCS,\n",
        "# because we already generated it for you, but feel free to uncomment it and execute it.\n",
        "# The key (service account (readerKey.json)) existing in the tutorial page may not have writing permissions to GCS.\n",
        "# Let us know if you want to be able to write files as well and we can issue you a different key.\n",
        "\n",
        "## Get training and testing data in dataframes in order to upload them to GCS\n",
        "#train_set = pd.DataFrame(x_over, columns=x.columns).join(pd.DataFrame(y_over, columns=[\"TARGET\"]))\n",
        "#test_set = pd.DataFrame(scaler_x_test, columns=x.columns).join(pd.DataFrame(scaler_y_test, columns=[\"TARGET\"]))\n",
        "#train_set.to_csv (r'gs://vectice-examples-samples/Customer_satisfaction_challenge/training_data.csv', index = False, header = True)\n",
        "#test_set.to_csv (r'gs://vectice-examples-samples/Customer_satisfaction_challenge/testing_data.csv', index = False, header = True)\n",
        "\n",
        "# We add new dataset versions \n",
        "train_ds_version = experiment.add_dataset_version(dataset=\"customer_satisfaction_training_dataset\")\n",
        "test_ds_version = experiment.add_dataset_version(dataset=\"customer_satisfaction_testing_dataset\")\n",
        "\n",
        "# We complete the current experiment's run \n",
        "## The added dataset versions will be automatically attached as outputs of the run\n",
        "### as they come after the start run and before the experiment.complete\n",
        "experiment.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e133e5",
      "metadata": {
        "id": "16e133e5"
      },
      "source": [
        "Our data contains now the same number of zeros and ones now"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d42f694c",
      "metadata": {
        "id": "d42f694c",
        "papermill": {
          "duration": 0.048004,
          "end_time": "2021-04-10T09:30:32.545240",
          "exception": false,
          "start_time": "2021-04-10T09:30:32.497236",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Modeling\n",
        "* LogisticRegression\n",
        "* LightGBM Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76a140a5",
      "metadata": {
        "id": "76a140a5"
      },
      "source": [
        "Here we create a function that calculates and shows the confusion matrix and the accuracy, precision, recall, f1_score, roc_auc metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afef1776",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:32.647997Z",
          "iopub.status.busy": "2021-04-10T09:30:32.647351Z",
          "iopub.status.idle": "2021-04-10T09:30:32.652002Z",
          "shell.execute_reply": "2021-04-10T09:30:32.651414Z"
        },
        "id": "afef1776",
        "papermill": {
          "duration": 0.05953,
          "end_time": "2021-04-10T09:30:32.652140",
          "exception": false,
          "start_time": "2021-04-10T09:30:32.592610",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_clf_eval(y_test, pred = None, pred_proba = None):\n",
        "    confusion = confusion_matrix(y_test, pred)\n",
        "    accuracy = accuracy_score(y_test, pred)\n",
        "    precision = precision_score(y_test, pred)\n",
        "    recall = recall_score(y_test, pred)\n",
        "    f1 = f1_score(y_test, pred)\n",
        "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
        "    \n",
        "    print('confusion')\n",
        "    print(confusion)\n",
        "    print('Accuacy : {}'.format(np.around(accuracy,4)))\n",
        "    print('Precision: {}'.format(np.around(precision,4)))\n",
        "    print('Recall : {}'.format(np.around(recall,4)))\n",
        "    print('F1 : {}'.format(np.around(f1,4)))  \n",
        "    print('ROC_AUC : {}'.format(np.around(roc_auc,4)))\n",
        "    return confusion, accuracy, precision, recall, f1, roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HoqX9ENCb4bA",
      "metadata": {
        "id": "HoqX9ENCb4bA"
      },
      "outputs": [],
      "source": [
        "# We create our second experiment for modeling and specify the workspace and the project we will be working on\n",
        "# Each experiment only contains one job. Each invokation of the job is called a run.\n",
        "# autocode = True enables you to track your git changes for your code automatically every time you execute a run (see below).\n",
        "experiment = Experiment(job=\"Modeling\", project=PROJECT_ID, job_type=JobType.TRAINING, auto_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jx4DNripQuWx",
      "metadata": {
        "id": "Jx4DNripQuWx"
      },
      "source": [
        "We can get the list of the models existing in our project by calling **vectice.list_models()**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BUaZ0R1cQuWy",
      "metadata": {
        "id": "BUaZ0R1cQuWy"
      },
      "outputs": [],
      "source": [
        "experiment.vectice.list_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f5f2f7a",
      "metadata": {
        "id": "2f5f2f7a",
        "papermill": {
          "duration": 0.047644,
          "end_time": "2021-04-10T09:30:32.748450",
          "exception": false,
          "start_time": "2021-04-10T09:30:32.700806",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "* **LogisticRegression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25VSc06-abyD",
      "metadata": {
        "id": "25VSc06-abyD"
      },
      "outputs": [],
      "source": [
        "## Logistic Regression\n",
        "# we declare the dataset versions and code to use as inputs of our run\n",
        "experiment.start(inputs=[train_ds_version,test_ds_version, input_code],\n",
        "                run_properties={\"Property1\": \"Value 1\", \"property2\": \"Value 2\"})\n",
        "\n",
        "lg_reg = LogisticRegression()\n",
        "\n",
        "lg_reg.fit(x_over, y_over)\n",
        "pred = lg_reg.predict(scaler_x_test)\n",
        "pred_proba = lg_reg.predict_proba(scaler_x_test)[:,1]\n",
        "\n",
        "confusion, accuracy, precision, recall, f1, roc_auc = get_clf_eval(scaler_y_test, pred=pred, pred_proba=pred_proba)\n",
        "    \n",
        "metrics = {'Accuracy score': round(accuracy, 3), \"Precision\": round(precision, 3),\n",
        "            \"Recall\": round(recall, 3), 'f1 score': round(f1, 3), 'AUC score': round(roc_auc, 3)}\n",
        "# We create a new model version \n",
        "model_version1 = experiment.add_model_version(\"Customer_Satisfaction_Classifier\", algorithm=\"Logistic Regression\", metrics=metrics)\n",
        "# We complete the current experiment's run \n",
        "## The created model version will be automatically attached as outputs of the run\n",
        "experiment.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b30a479",
      "metadata": {
        "id": "4b30a479",
        "papermill": {
          "duration": 0.048212,
          "end_time": "2021-04-10T09:30:40.141181",
          "exception": false,
          "start_time": "2021-04-10T09:30:40.092969",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "* **LightGBM Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1746651",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:40.247991Z",
          "iopub.status.busy": "2021-04-10T09:30:40.246574Z",
          "iopub.status.idle": "2021-04-10T09:30:40.269645Z",
          "shell.execute_reply": "2021-04-10T09:30:40.270257Z"
        },
        "id": "e1746651",
        "papermill": {
          "duration": 0.079436,
          "end_time": "2021-04-10T09:30:40.270446",
          "exception": false,
          "start_time": "2021-04-10T09:30:40.191010",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "scaler_x_test, scaler_x_val, scaler_y_test, scaler_y_val = train_test_split(scaler_x_test, scaler_y_test, test_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73fc68d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-10T09:30:40.373748Z",
          "iopub.status.busy": "2021-04-10T09:30:40.372987Z",
          "iopub.status.idle": "2021-04-10T09:30:40.377244Z",
          "shell.execute_reply": "2021-04-10T09:30:40.376801Z"
        },
        "id": "f73fc68d",
        "papermill": {
          "duration": 0.058534,
          "end_time": "2021-04-10T09:30:40.377387",
          "exception": false,
          "start_time": "2021-04-10T09:30:40.318853",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "##Setting up the model's parameters\n",
        "## Feel free to play with the parameters\n",
        "train_data = lgb.Dataset(x_over, label=y_over)\n",
        "val_data = lgb.Dataset(scaler_x_val, label=scaler_y_val)\n",
        "n_estimators = 5000\n",
        "num_leaves = 20\n",
        "min_data_in_leaf = 80\n",
        "learning_rate = 0.001\n",
        "boosting = 'gbdt'\n",
        "objective = 'binary'\n",
        "metric = 'auc'\n",
        "params = {\n",
        "    'n_estimators': n_estimators,\n",
        "    'num_leaves': num_leaves,\n",
        "    'min_data_in_leaf': min_data_in_leaf,\n",
        "    'learning_rate': learning_rate,\n",
        "    'boosting': boosting,\n",
        "    'objective': objective,\n",
        "    'metric': metric,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cLn__vFAbYfK",
      "metadata": {
        "id": "cLn__vFAbYfK"
      },
      "outputs": [],
      "source": [
        "## LightGBM Classifier\n",
        "# we declare the dataset versions and code to use as inputs of our run\n",
        "experiment.start(inputs=[train_ds_version,test_ds_version, input_code], run_properties={\"Property1\": \"Value 1\", \"property2\": \"Value 2\"})\n",
        "\n",
        "lgbm = lgb.train(params,\n",
        "                  train_data,\n",
        "                  valid_sets=val_data, \n",
        "                  valid_names=['train','valid'],\n",
        "                  early_stopping_rounds=300)\n",
        "\n",
        "# Predicting the output on the Test Dataset \n",
        "ypred_lgbm = lgbm.predict(scaler_x_test)\n",
        "ypred_lgbm\n",
        "y_pred_lgbm_class = [np.argmax(line) for line in ypred_lgbm]\n",
        "accuracy_lgbm=accuracy_score(scaler_y_test,y_pred_lgbm_class)\n",
        "print(accuracy_lgbm)\n",
        "#Print Area Under Curve\n",
        "plt.figure()\n",
        "false_positive_rate, recall, thresholds = roc_curve(scaler_y_test, ypred_lgbm)\n",
        "roc_auc = auc(false_positive_rate, recall)\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot([0,1], [0,1], 'r--')\n",
        "plt.xlim([0.0,1.0])\n",
        "plt.ylim([0.0,1.0])\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Fall-out (1-Specificity)')\n",
        "plt.savefig(\"ROC_curve.png\")\n",
        "plt.show()\n",
        "print('AUC score:', roc_auc)\n",
        "\n",
        "metrics = {\"Accuracy score\": round(accuracy_lgbm, 3), \"AUC score\": round(roc_auc, 3)} \n",
        "hyper_parameters =  {\"n_estimators\": n_estimators, \"num_leaves\": num_leaves,\n",
        "              \"min_data_in_leaf\": min_data_in_leaf, \"learning_rate\": learning_rate, \"boosting\": boosting,\n",
        "              \"objective\": objective, \"metric\": metric}\n",
        "# We create a new model version \n",
        "model_version2 = experiment.add_model_version(model=\"Customer_Satisfaction_Classifier\", algorithm=\"Light GBM\", metrics=metrics,\n",
        "                                             hyper_parameters=hyper_parameters, attachment=[\"ROC_curve.png\"])\n",
        "# We complete the current experiment's run \n",
        "## The created model version will be automatically attached as outputs of the run\n",
        "experiment.complete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eb29db7c",
        "e786cc41"
      ],
      "name": "customer_satisfaction_challenge.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "7eb6f6862a6947ac5369bdb367f27d42e7da01a01c09c3d67648c4373e3c8da1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 162.044116,
      "end_time": "2021-04-10T09:32:31.418407",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-04-10T09:29:49.374291",
      "version": "2.3.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
