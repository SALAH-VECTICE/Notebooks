{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vectice/vectice-examples/blob/master/Samples/Amazon_access_challenge/Amazon_employee_access_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpmSSdxOTISb",
        "papermill": {
          "duration": 0.045127,
          "end_time": "2020-10-15T16:08:16.602733",
          "exception": false,
          "start_time": "2020-10-15T16:08:16.557606",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Amazon Employee Access Challenge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGo38hdyTISe",
        "papermill": {
          "duration": 0.043196,
          "end_time": "2020-10-15T16:08:16.691894",
          "exception": false,
          "start_time": "2020-10-15T16:08:16.648698",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Frame the Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIo3BkXnTISg",
        "papermill": {
          "duration": 0.042689,
          "end_time": "2020-10-15T16:08:16.777892",
          "exception": false,
          "start_time": "2020-10-15T16:08:16.735203",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "- The given problem is related with time wasted in granting and revoking access to the employee within company.  For employee to access any resources he/she needs prior permission i.e. access of that resource. The access granting and revoking process is manual, handled by superviso. As employees move throughout a company, this access discovery/recovery cycle wastes a nontrivial amount of time and money.\n",
        "\n",
        "- <b>Objective:</b> We have to build a model, learned using historical data, that will determine an employee's access needs, such that manual access transactions (grants and revokes) are minimized as the employee's attributes change over time. The model will take an employee's role information and a resource code and will return whether or not access should be granted.\n",
        "\n",
        "\n",
        "- <b>Data:</b> The data consists of real historical data collected from 2010 & 2011. Employees are manually allowed or denied access to resources over time. You must create an algorithm capable of learning from this historical data to predict approval/denial for an unseen set of employees.\n",
        "\n",
        "Test dataset (10 columns): The test set for which predictions should be made.  Each row asks whether an employee having the listed characteristics should have access to the listed resource.\n",
        "\n",
        "Training dataset (10 columns): Each row has the ACTION (ground truth), RESOURCE, and information about the employee's role at the time of approval.\n",
        "Following are the features present in the training dataset:\n",
        "- ACTION: Target variable. ACTION is 1 if the resource was approved, 0 if the resource was not approved.\n",
        "- RESOURCE: An ID for each resource\n",
        "- MGR_ID: The EMPLOYEE ID of the manager of the current EMPLOYEE ID record; an employee may have only one manager at a time\n",
        "- ROLE_ROLLUP_1: Company role grouping category id 1 (e.g. US Engineering)\n",
        "- ROLE_ROLLUP_2: Company role grouping category id 2 (e.g. US Retail)\n",
        "- ROLE_DEPTNAME: Company role department name (e.g. Retail)\n",
        "- ROLE_TITLE: Company role business title description (e.g. Senior Engineering Retail Manager)\n",
        "- ROLE_FAMILY_DESC: Company role family extended description (e.g. Retail Manager, Software Engineering)\n",
        "- ROLE_FAMILY: Company role family description (e.g. Retail Manager)\n",
        "- ROLE_CODE: Company role code; this code is unique to each role (e.g. Manager)\n",
        "\n",
        "All features have numerical values but they all are categorical features.\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KnR-E6tTdK6"
      },
      "source": [
        "## Install Vectice and GCS packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iWoITyrtEZm"
      },
      "source": [
        "Vectice provides a generic metadata layer that is potentially suitable for most data science workflows. For this notebook we will use the sickit-learn library for modeling and track experiments directly through our Python SDK to illustrate how to fine-tune exactly what you would like to track: metrics, etc. The same mechanisms would apply to R, Java or even more generic REST APIs to track metadata from any programming language and library.\n",
        "\n",
        "Here is a link to the [Vectice Python library documentation](https://doc.vectice.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pIWktunaV0I",
        "outputId": "30b2cf50-6780-4a64-9814-2e21663ac4f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 136 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 39.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 121 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 291 kB 40.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 856 kB 40.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "## Requirements\n",
        "!pip install --q fsspec\n",
        "!pip install --q gcsfs\n",
        "#Install Vectice Python library \n",
        "# In this tutorial we will do code versioning using github, we also support gitlab\n",
        "# and bitbucket: !pip install -q \"vectice[github, gitlab, bitbucket]\"\n",
        "!pip install --q vectice[github]==22.3.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lQSTHHKaV0J"
      },
      "outputs": [],
      "source": [
        "!pip show vectice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdspQJQOaV0K"
      },
      "source": [
        "## Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyqw-E-WaV0K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkth-sSZTuo2"
      },
      "source": [
        "## Get the data from GCS "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lerYMowcxhl"
      },
      "source": [
        "We are going to load data stored in Google Cloud Storage, that is provided by Vectice for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5FjqgyjT09K"
      },
      "outputs": [],
      "source": [
        "# Download the \"JSON file\" from the \"Vectice Tutorial Page\" in the application so that \n",
        "# you can access the GCS bucket. The name of the JSON file should be \"readerKey.json\"\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oApCUClIDLhA"
      },
      "outputs": [],
      "source": [
        "# Once your file is loaded set the credentials for GCS and load the file\n",
        "# in a pandas frame, double check the json file name you uploaded.\n",
        "\n",
        "### Complete with the name of the JSON key file to access GCS. It can be found in the tutorial page.\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'readerKey.json'\n",
        "\n",
        "#  Get the dataset from GCS\n",
        "data = pd.read_csv('gs://vectice-examples-samples/Amazon_challenge/dataset.csv')\n",
        "# Run head to make sure the data was loaded properly\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:18.038159Z",
          "iopub.status.busy": "2020-10-15T16:08:18.036943Z",
          "iopub.status.idle": "2020-10-15T16:08:18.144186Z",
          "shell.execute_reply": "2020-10-15T16:08:18.143571Z"
        },
        "id": "H8OIGy2dTISl",
        "papermill": {
          "duration": 0.160527,
          "end_time": "2020-10-15T16:08:18.144317",
          "exception": false,
          "start_time": "2020-10-15T16:08:17.983790",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xklmP-FXTISq",
        "papermill": {
          "duration": 0.055283,
          "end_time": "2020-10-15T16:08:18.245037",
          "exception": false,
          "start_time": "2020-10-15T16:08:18.189754",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-NKIjrWUCS0"
      },
      "source": [
        "Data exploration enables us to take a first look on the data, can enhance the overall understanding of the characteristics of the data domain and helps to detect correlation between the features, thereby allowing for the creation of more accurate models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:18.349444Z",
          "iopub.status.busy": "2020-10-15T16:08:18.348286Z",
          "iopub.status.idle": "2020-10-15T16:08:18.353917Z",
          "shell.execute_reply": "2020-10-15T16:08:18.353247Z"
        },
        "id": "j31At0hlTISr",
        "papermill": {
          "duration": 0.063886,
          "end_time": "2020-10-15T16:08:18.354043",
          "exception": false,
          "start_time": "2020-10-15T16:08:18.290157",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_explore = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:18.453482Z",
          "iopub.status.busy": "2020-10-15T16:08:18.452289Z",
          "iopub.status.idle": "2020-10-15T16:08:18.467495Z",
          "shell.execute_reply": "2020-10-15T16:08:18.466716Z"
        },
        "id": "Y0mrxzGWTISs",
        "papermill": {
          "duration": 0.068238,
          "end_time": "2020-10-15T16:08:18.467626",
          "exception": false,
          "start_time": "2020-10-15T16:08:18.399388",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_explore.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQas3mmsTISt",
        "papermill": {
          "duration": 0.051153,
          "end_time": "2020-10-15T16:08:18.564995",
          "exception": false,
          "start_time": "2020-10-15T16:08:18.513842",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "There is no column with null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:18.673017Z",
          "iopub.status.busy": "2020-10-15T16:08:18.671897Z",
          "iopub.status.idle": "2020-10-15T16:08:18.688578Z",
          "shell.execute_reply": "2020-10-15T16:08:18.687779Z"
        },
        "id": "NmC-oL2DTISu",
        "papermill": {
          "duration": 0.07767,
          "end_time": "2020-10-15T16:08:18.688714",
          "exception": false,
          "start_time": "2020-10-15T16:08:18.611044",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_explore.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqJ4dAT3TISv",
        "papermill": {
          "duration": 0.0458,
          "end_time": "2020-10-15T16:08:18.781154",
          "exception": false,
          "start_time": "2020-10-15T16:08:18.735354",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "- In dataset description it is mention that an employee can have only one manager at a time, then we can consider that the dataset contains information of maximum 4243 employees.\n",
        "- There are same number of unique values for ROLE_TITLE and ROLE_CODE. There is 1-to-1 mapping between these columns. So for our problem only one feature is sufficent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:18.885058Z",
          "iopub.status.busy": "2020-10-15T16:08:18.884258Z",
          "iopub.status.idle": "2020-10-15T16:08:19.059166Z",
          "shell.execute_reply": "2020-10-15T16:08:19.058398Z"
        },
        "id": "WmmQgxxqTISw",
        "papermill": {
          "duration": 0.227781,
          "end_time": "2020-10-15T16:08:19.059318",
          "exception": false,
          "start_time": "2020-10-15T16:08:18.831537",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='ACTION', data=data_explore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40dN9NYNUc5m"
      },
      "outputs": [],
      "source": [
        "data['ACTION'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOvXAqszTISx",
        "papermill": {
          "duration": 0.048468,
          "end_time": "2020-10-15T16:08:19.155982",
          "exception": false,
          "start_time": "2020-10-15T16:08:19.107514",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "- We can see that we have an imbalanced dataset. There are very less records of not granting the access. some algorithms may learn just from the ones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqlvAQvzUUDV"
      },
      "outputs": [],
      "source": [
        "## Manager ID and how much resources he has access to\n",
        "data['MGR_ID'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jF2Iw9BTISy",
        "papermill": {
          "duration": 0.047035,
          "end_time": "2020-10-15T16:08:19.252507",
          "exception": false,
          "start_time": "2020-10-15T16:08:19.205472",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "- Lets find out top 15 Resources, Role department, Role family, Role codes for which most access is requested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:19.357927Z",
          "iopub.status.busy": "2020-10-15T16:08:19.356856Z",
          "iopub.status.idle": "2020-10-15T16:08:19.376182Z",
          "shell.execute_reply": "2020-10-15T16:08:19.375355Z"
        },
        "id": "bo5oEnFXTISz",
        "papermill": {
          "duration": 0.07575,
          "end_time": "2020-10-15T16:08:19.376315",
          "exception": false,
          "start_time": "2020-10-15T16:08:19.300565",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_explore_resources = data_explore[['RESOURCE', \"ACTION\"]].groupby(by='RESOURCE').count()\n",
        "data_explore_resources.sort_values('ACTION', ascending=False).head(n=15).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:19.483038Z",
          "iopub.status.busy": "2020-10-15T16:08:19.481908Z",
          "iopub.status.idle": "2020-10-15T16:08:19.498392Z",
          "shell.execute_reply": "2020-10-15T16:08:19.497728Z"
        },
        "id": "rNMwE8DMTIS0",
        "papermill": {
          "duration": 0.073718,
          "end_time": "2020-10-15T16:08:19.498518",
          "exception": false,
          "start_time": "2020-10-15T16:08:19.424800",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_explore_role_dept = data_explore[['ROLE_DEPTNAME', \"ACTION\"]].groupby(by='ROLE_DEPTNAME').count()\n",
        "data_explore_role_dept.sort_values('ACTION', ascending=False).head(n=15).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:19.606351Z",
          "iopub.status.busy": "2020-10-15T16:08:19.605555Z",
          "iopub.status.idle": "2020-10-15T16:08:19.622097Z",
          "shell.execute_reply": "2020-10-15T16:08:19.621236Z"
        },
        "id": "F2c1zQSlTIS1",
        "papermill": {
          "duration": 0.074037,
          "end_time": "2020-10-15T16:08:19.622232",
          "exception": false,
          "start_time": "2020-10-15T16:08:19.548195",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_explore_role_codes = data_explore[['ROLE_CODE', \"ACTION\"]].groupby(by='ROLE_CODE').count()\n",
        "data_explore_role_codes.sort_values('ACTION', ascending=False).head(n=15).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:19.745722Z",
          "iopub.status.busy": "2020-10-15T16:08:19.744459Z",
          "iopub.status.idle": "2020-10-15T16:08:19.769136Z",
          "shell.execute_reply": "2020-10-15T16:08:19.770234Z"
        },
        "id": "XZk0Ts1mTIS1",
        "papermill": {
          "duration": 0.09194,
          "end_time": "2020-10-15T16:08:19.770470",
          "exception": false,
          "start_time": "2020-10-15T16:08:19.678530",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_explore_role_family = data_explore[['ROLE_FAMILY', \"ACTION\"]].groupby(by='ROLE_FAMILY').count()\n",
        "data_explore_role_family.sort_values('ACTION', ascending=False).head(n=15).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvpiYy2qUybn"
      },
      "outputs": [],
      "source": [
        "## We use data.describe() to only take numerical columns ,and avoid non numerical ones, in order to plot them\n",
        "for i in data.describe().columns:\n",
        "  sns.distplot(data[i].dropna())\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKL7m-3hskLx"
      },
      "source": [
        "### Correlation\n",
        "\n",
        "If we have a big correlation, we have a problem of multicolinearity. That means that there are some features that depend of other features, so we should reduce the dimentionality of our data (if A depends of B, we should either find a way to aggregate or combine the two features and turn it into one variable or drop one of the variables that are too highly correlated with another) and that can be adressed using Principal component analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:19.888530Z",
          "iopub.status.busy": "2020-10-15T16:08:19.887676Z",
          "iopub.status.idle": "2020-10-15T16:08:20.513889Z",
          "shell.execute_reply": "2020-10-15T16:08:20.513220Z"
        },
        "id": "6Vz3uKF0TIS2",
        "papermill": {
          "duration": 0.684604,
          "end_time": "2020-10-15T16:08:20.514017",
          "exception": false,
          "start_time": "2020-10-15T16:08:19.829413",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "## If we have a big correlation, we have a problem of multicolinearity that can be adressed using PCA\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(data.corr(),annot=True,cmap='viridis',linewidth=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:20.630370Z",
          "iopub.status.busy": "2020-10-15T16:08:20.629477Z",
          "iopub.status.idle": "2020-10-15T16:08:20.634298Z",
          "shell.execute_reply": "2020-10-15T16:08:20.633513Z"
        },
        "id": "JD2HZ9E_TIS3",
        "papermill": {
          "duration": 0.066424,
          "end_time": "2020-10-15T16:08:20.634454",
          "exception": false,
          "start_time": "2020-10-15T16:08:20.568030",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "corr_matrix = data_explore.corr()\n",
        "corr_matrix['ACTION'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhWY7tLrTIS4",
        "papermill": {
          "duration": 0.060208,
          "end_time": "2020-10-15T16:08:20.758519",
          "exception": false,
          "start_time": "2020-10-15T16:08:20.698311",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "- There is no attribute to which target variable is strongly correlated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwHD-UTNVNkj"
      },
      "source": [
        "## Vectice Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGuJnWMKt2RD"
      },
      "outputs": [],
      "source": [
        "from vectice import Experiment\n",
        "from vectice.api.json import JobType\n",
        "from vectice.api.json import ModelType\n",
        "\n",
        "# Specify the API endpoint for Vectice.\n",
        "# You can specify your API endpoint here in the notebook, but we recommand you to add it to a .env file\n",
        "os.environ['VECTICE_API_ENDPOINT']= \"app.vectice.com\"\n",
        "\n",
        "# To use the Vectice Python library, you first need to authenticate your account using an API token.\n",
        "# You can generate an API token from the Vectice UI, by going to the \"API Tokens\" section in the \"My Profile\" section\n",
        "# which is located under your profile picture.\n",
        "# You can specify your API Token here in the notebook, but we recommend you to add it to a .env file\n",
        "os.environ['VECTICE_API_TOKEN'] = \"Your API Token\"\n",
        "\n",
        "# Add you project id. The project id can be found in the project settings page in the Vectice UI\n",
        "project_id = ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V487vfI9TIS4",
        "papermill": {
          "duration": 0.054109,
          "end_time": "2020-10-15T16:08:20.866855",
          "exception": false,
          "start_time": "2020-10-15T16:08:20.812746",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:20.982290Z",
          "iopub.status.busy": "2020-10-15T16:08:20.981324Z",
          "iopub.status.idle": "2020-10-15T16:08:21.332009Z",
          "shell.execute_reply": "2020-10-15T16:08:21.331208Z"
        },
        "id": "3I9Y_KOVTIS5",
        "papermill": {
          "duration": 0.411914,
          "end_time": "2020-10-15T16:08:21.332141",
          "exception": false,
          "start_time": "2020-10-15T16:08:20.920227",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:21.454139Z",
          "iopub.status.busy": "2020-10-15T16:08:21.453232Z",
          "iopub.status.idle": "2020-10-15T16:08:21.463067Z",
          "shell.execute_reply": "2020-10-15T16:08:21.462402Z"
        },
        "id": "oBoMqRUVTIS5",
        "papermill": {
          "duration": 0.074693,
          "end_time": "2020-10-15T16:08:21.463196",
          "exception": false,
          "start_time": "2020-10-15T16:08:21.388503",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "X = data.drop(columns=['ACTION'], axis=1).copy()\n",
        "y = data['ACTION'].copy()\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:21.580097Z",
          "iopub.status.busy": "2020-10-15T16:08:21.578793Z",
          "iopub.status.idle": "2020-10-15T16:08:21.583477Z",
          "shell.execute_reply": "2020-10-15T16:08:21.582836Z"
        },
        "id": "Gvn3YgvfTIS6",
        "papermill": {
          "duration": 0.066358,
          "end_time": "2020-10-15T16:08:21.583608",
          "exception": false,
          "start_time": "2020-10-15T16:08:21.517250",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "cat_attrs = list(X.columns)\n",
        "cat_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eHC-oLRjZpX"
      },
      "outputs": [],
      "source": [
        "# We create our first experiment for data preparation and specify the workspace and the project we will be working on\n",
        "# Each experiment only contains one job. Each invokation of the job is called a run.\n",
        "# autocode = True enables you to track your git changes for your code automatically every time you execute a run (see below).\n",
        "experiment = Experiment(job=\"jobSplitData\", job_type = JobType.PREPARATION, project=project_id, auto_code = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jgyZBYEaV0R"
      },
      "source": [
        "We can check if the datasets are already created in our workspace by calling **experiment.vectice.list_datasets()** which lists all the datasets existing in the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a44GR7gYaV0R"
      },
      "outputs": [],
      "source": [
        "experiment.vectice.list_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwx1HlUZvP2R"
      },
      "source": [
        "Let's split the dataset into train and test sets and save them in GCS. (The GCS code has been commented out as the data has already been generated). For this section, we will re-use some datasets that have been already created to illustrate dataset versioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrmRlyIhtzOr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# We use auto-versioning here.\n",
        "# The Vectice library automatically detects if there have been changes to the dataset you are using.\n",
        "# If it detects changes, it will generate a new version of your dataset automatically.\n",
        "# For this notebook, we changed the data to illustrate datasets auto-versioning..\n",
        "# So, the Vectice Python library will create a new dataset version when this code is executed for the first time.\n",
        "input_ds_version =  experiment.add_dataset_version(\"amazon_employee_access_challenge_dataset\")\n",
        "\n",
        "# Because we are using Colab in this tutorial example we are going to declare a reference to the code\n",
        "## manually. This will be added as a reference to the run we are going to create next.\n",
        "# If you are using your local environment with GIT installed or JupyterLab etc... the code\n",
        "# tracking is automated.\n",
        "uri = \"https://github.com/vectice/vectice-examples\"\n",
        "entrypoint=\"Samples/Amazon_access_challenge/Amazon_employee_access_challenge.ipynb\"\n",
        "input_code = experiment.add_code_version_uri(git_uri=uri, entrypoint=entrypoint)\n",
        "\n",
        "# The created dataset version and code version will be automatically attached as inputs of the run\n",
        "experiment.start(run_properties={\"Property1\": \"Value 1\", \"property2\": \"Value 2\"})\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "\n",
        "for train_index, test_index in split.split(X, y):\n",
        "    strat_train_set = data.iloc[train_index]\n",
        "    strat_test_set = data.iloc[test_index]\n",
        "\n",
        "X_train = strat_train_set.drop('ACTION', axis=1)\n",
        "y_train = strat_train_set['ACTION'].copy()\n",
        "X_test = strat_test_set.drop('ACTION', axis=1)\n",
        "y_test = strat_test_set['ACTION'].copy()\n",
        "X_train.shape, X_test.shape\n",
        "\n",
        "train_set = X_train.join(y_train)\n",
        "test_set = X_test.join(y_test)\n",
        "\n",
        "# We commented out the code to persist the training and testing test in GCS,\n",
        "# because we already generated it for you, but feel free to uncomment it and execute it.\n",
        "# The key (service account (readerKey.json)) existing in the tutorial page may not have writing permissions to GCS.\n",
        "# Let us know if you want to be able to write files as well and we can issue you a different key.\n",
        "\n",
        "#train_set.to_csv (r'gs://vectice-examples-samples/Amazon_challenge/training_data.csv', index = False, header = True)\n",
        "#test_set.to_csv (r'gs://vectice-examples-samples/Amazon_challenge/testing_data.csv', index = False, header = True)\n",
        "\n",
        "# We create new dataset versions \n",
        "train_ds_version = experiment.add_dataset_version(\"Training_data_Amazon\")\n",
        "test_ds_version = experiment.add_dataset_version(\"Testing_data_Amazon\")\n",
        "\n",
        "# We complete the current experiment's run \n",
        "## The created dataset versions will be automatically attached as outputs of the run\n",
        "experiment.complete()\n",
        "\n",
        "# We can preview one of our generated outputs to make sure that everything was executed properly.\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zywmsFo5tmRt"
      },
      "source": [
        "We create a pipeline with the OneHotEncoder for algorithms that doesn't support categorical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:21.887013Z",
          "iopub.status.busy": "2020-10-15T16:08:21.886124Z",
          "iopub.status.idle": "2020-10-15T16:08:21.989771Z",
          "shell.execute_reply": "2020-10-15T16:08:21.989065Z"
        },
        "id": "BBweUu04TIS7",
        "papermill": {
          "duration": 0.175769,
          "end_time": "2020-10-15T16:08:21.989899",
          "exception": false,
          "start_time": "2020-10-15T16:08:21.814130",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "                         ('cat_enc', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "pre_process = ColumnTransformer([('cat_process', cat_pipeline, cat_attrs)], remainder='passthrough')\n",
        "\n",
        "X_train_transformed = pre_process.fit_transform(X_train)\n",
        "X_test_transformed = pre_process.transform(X_test)\n",
        "X_train_transformed.shape, X_test_transformed.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYCoaHjiTIS8",
        "papermill": {
          "duration": 0.056567,
          "end_time": "2020-10-15T16:08:22.105733",
          "exception": false,
          "start_time": "2020-10-15T16:08:22.049166",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "- Since we will be using CatBoost Classifier. For CatBoost model, there is no need of encoding categorical model. Hence we will be creating a separate preprocessing pipeline for CatBoost model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:22.231547Z",
          "iopub.status.busy": "2020-10-15T16:08:22.230453Z",
          "iopub.status.idle": "2020-10-15T16:08:22.262458Z",
          "shell.execute_reply": "2020-10-15T16:08:22.261775Z"
        },
        "id": "EgmwvQCITIS8",
        "papermill": {
          "duration": 0.100032,
          "end_time": "2020-10-15T16:08:22.262592",
          "exception": false,
          "start_time": "2020-10-15T16:08:22.162560",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "cat_boost_pre_process = ColumnTransformer([('imputer', SimpleImputer(strategy='most_frequent'), cat_attrs)], remainder='passthrough')\n",
        "\n",
        "X_cb_train_transformed = cat_boost_pre_process.fit_transform(X_train)\n",
        "X_cb_test_transformed = cat_boost_pre_process.transform(X_test)\n",
        "X_cb_train_transformed.shape, X_cb_test_transformed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:22.413099Z",
          "iopub.status.busy": "2020-10-15T16:08:22.412209Z",
          "iopub.status.idle": "2020-10-15T16:08:22.436604Z",
          "shell.execute_reply": "2020-10-15T16:08:22.435864Z"
        },
        "id": "LeqMeHWATIS9",
        "papermill": {
          "duration": 0.108777,
          "end_time": "2020-10-15T16:08:22.436774",
          "exception": false,
          "start_time": "2020-10-15T16:08:22.327997",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "feature_columns = list(pre_process.transformers_[0][1]['cat_enc'].get_feature_names(cat_attrs))\n",
        "len(feature_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfFabR0LTIS9",
        "papermill": {
          "duration": 0.063239,
          "end_time": "2020-10-15T16:08:22.561166",
          "exception": false,
          "start_time": "2020-10-15T16:08:22.497927",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#We create our second experiment for Modeling and specify the workspace and the project we will be working on\n",
        "#Each experiment only contains one job. Each invokation of the job is called a run.\n",
        "#autocode = True enables you to track your git changes for your code automatically every time you execute a run (see below).\n",
        "experiment = Experiment(job=\"Modeling\", project=project_id, job_type=JobType.TRAINING, auto_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:22.828098Z",
          "iopub.status.busy": "2020-10-15T16:08:22.827276Z",
          "iopub.status.idle": "2020-10-15T16:08:22.831325Z",
          "shell.execute_reply": "2020-10-15T16:08:22.830571Z"
        },
        "id": "H6RaYtvwTIS9",
        "papermill": {
          "duration": 0.068427,
          "end_time": "2020-10-15T16:08:22.831518",
          "exception": false,
          "start_time": "2020-10-15T16:08:22.763091",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:23.063376Z",
          "iopub.status.busy": "2020-10-15T16:08:23.062573Z",
          "iopub.status.idle": "2020-10-15T16:08:23.066395Z",
          "shell.execute_reply": "2020-10-15T16:08:23.065787Z"
        },
        "id": "JA65CUmOTIS-",
        "papermill": {
          "duration": 0.176391,
          "end_time": "2020-10-15T16:08:23.066527",
          "exception": false,
          "start_time": "2020-10-15T16:08:22.890136",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef, make_scorer, roc_auc_score, roc_curve\n",
        "Matthew = make_scorer(matthews_corrcoef)\n",
        "\n",
        "results = []\n",
        "\n",
        "def plot_custom_roc_curve(clf_name, y_true, y_scores):\n",
        "    auc_score = np.round(roc_auc_score(y_true, y_scores), 3)\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=clf_name+\" (AUC Score: {})\".format(str(auc_score)))\n",
        "    plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal\n",
        "    plt.axis([0, 1, 0, 1])\n",
        "    plt.xlabel(\"FPR\", fontsize=16)\n",
        "    plt.ylabel(\"TPR\", fontsize=16)\n",
        "    plt.legend()\n",
        "    \n",
        "    \n",
        "def performance_measures(model, X_tr=X_train_transformed, y_tr=y_train, X_ts=X_test_transformed, y_ts=y_test,\n",
        "                         store_results=True):\n",
        "    train_mcc = cross_val_score(model, X_tr, y_tr, scoring=Matthew, cv=kf, n_jobs=-1)\n",
        "    test_mcc = cross_val_score(model, X_ts, y_ts, scoring=Matthew, cv=kf, n_jobs=-1)\n",
        "    print(\"Mean Train MCC: {}\\nMean Test MCC: {}\".format(train_mcc.mean(), test_mcc.mean()))\n",
        "\n",
        "    \n",
        "    train_roc_auc = cross_val_score(model, X_tr, y_tr, scoring='roc_auc', cv=kf, n_jobs=-1)\n",
        "    test_roc_auc = cross_val_score(model, X_ts, y_ts, scoring='roc_auc', cv=kf, n_jobs=-1)\n",
        "    print(\"Mean Train ROC AUC Score: {}\\nMean Test ROC AUC Score: {}\".format(train_roc_auc.mean(), test_roc_auc.mean()))\n",
        "    return train_mcc.mean(), test_mcc.mean(), train_roc_auc.mean(), test_roc_auc.mean()\n",
        "    \n",
        "    if store_results:\n",
        "        results.append([model.__class__.__name__, np.round(np.mean(train_roc_auc), 3), np.round(np.mean(test_roc_auc), 3), np.round(np.mean(train_mcc), 3), np.round(np.mean(test_mcc), 3)])\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:23.195123Z",
          "iopub.status.busy": "2020-10-15T16:08:23.193982Z",
          "iopub.status.idle": "2020-10-15T16:08:23.197578Z",
          "shell.execute_reply": "2020-10-15T16:08:23.196821Z"
        },
        "id": "CIbGOO9oTIS-",
        "papermill": {
          "duration": 0.072418,
          "end_time": "2020-10-15T16:08:23.197707",
          "exception": false,
          "start_time": "2020-10-15T16:08:23.125289",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def plot_feature_importance(feature_columns, importance_values, top_n_features=10):\n",
        "    feature_imp = [ col for col in zip(feature_columns, importance_values)]\n",
        "    feature_imp.sort(key=lambda x:x[1], reverse=True)\n",
        "    \n",
        "    if top_n_features:\n",
        "        imp = pd.DataFrame(feature_imp[0:top_n_features], columns=['feature', 'importance'])\n",
        "    else:\n",
        "        imp = pd.DataFrame(feature_imp, columns=['feature', 'importance'])\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    sns.barplot(y='feature', x='importance', data=imp, orient='h')\n",
        "    plt.title('Most Important Features', fontsize=16)\n",
        "    plt.ylabel(\"Feature\", fontsize=16)\n",
        "    plt.xlabel(\"\")\n",
        "    plt.savefig('Feature_importance.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rRVYLD5aV0T"
      },
      "source": [
        "We can get the list of the models existing in the project by calling **vectice.list_models()**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmRNVdEZaV0U"
      },
      "outputs": [],
      "source": [
        "experiment.vectice.list_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzIKy37qu8Ha"
      },
      "source": [
        "### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKMOepa4vJiS"
      },
      "outputs": [],
      "source": [
        "#Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# we declare the dataset versions and code to use as inputs of our run\n",
        "experiment.start(inputs=[input_code, train_ds_version, test_ds_version],\n",
        "                run_properties={\"Property1\": \"Value 1\", \"property2\": \"Value 2\"})\n",
        "\n",
        "solver='liblinear'\n",
        "C=1\n",
        "penalty='l2'\n",
        "max_iter=1000\n",
        "random_state=42\n",
        "logistic_reg = LogisticRegression(solver=solver, C=C, penalty=penalty, max_iter=max_iter, random_state=random_state)\n",
        "logistic_reg.fit(X_train_transformed, y_train)\n",
        "train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(logistic_reg)\n",
        "plot_feature_importance(feature_columns, logistic_reg.coef_[0], top_n_features=15)\n",
        "\n",
        "metrics = {\"Train_mcc\":  round(train_mcc, 3),\"test_mcc\":  round(test_mcc, 3), \"train_roc_auc\":  round(train_roc_auc, 3), 'test_roc_auc':  round(test_roc_auc, 3)}\n",
        "hyper_parameters = {\"solver\": solver, \"C\": C, \"penalty\": penalty, \"max_iter\": max_iter, \"random_state\": random_state}\n",
        "\n",
        "# Let's log the model we trained along with its metrics, as a new version \n",
        "# of the \"Classifier\" model in Vectice.\n",
        "model_version = experiment.add_model_version(model=\"Classifier\", algorithm=\"Logistic Regression\", hyper_parameters=hyper_parameters, metrics=metrics, attachment=\"Feature_importance.png\")\n",
        "\n",
        "# We complete the current experiment's run \n",
        "## The created model version will be automatically attached as output of the run\n",
        "experiment.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9icQG8w7vAUc"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:08:27.787024Z",
          "iopub.status.busy": "2020-10-15T16:08:27.786201Z",
          "iopub.status.idle": "2020-10-15T16:08:30.387017Z",
          "shell.execute_reply": "2020-10-15T16:08:30.387601Z"
        },
        "id": "7lISF6y2TITA",
        "papermill": {
          "duration": 2.66877,
          "end_time": "2020-10-15T16:08:30.387773",
          "exception": false,
          "start_time": "2020-10-15T16:08:27.719003",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# we declare the dataset versions and code to use as inputs of our run\n",
        "experiment.start(inputs=[input_code, train_ds_version, test_ds_version],\n",
        "                run_properties={\"Property1\": \"Value 1\", \"property2\": \"Value 2\"})\n",
        "\n",
        "n_estimators=300\n",
        "max_depth=16\n",
        "random_state=42\n",
        "\n",
        "forest_clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
        "forest_clf.fit(X_train_transformed, y_train)\n",
        "\n",
        "train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(forest_clf)\n",
        "\n",
        "metrics = {\"train_roc_auc\": round(train_roc_auc, 3), 'test_roc_auc': round(test_roc_auc, 3)}\n",
        "hyper_parameters = {\"n_estimators\": n_estimators, \"max_depth\": max_depth, \"random_state\": random_state}\n",
        "plot_feature_importance(feature_columns, forest_clf.feature_importances_, top_n_features=15)\n",
        "\n",
        "# Let's log the model we trained along with its metrics, as a new version \n",
        "# of the \"Classifier\" model in Vectice.\n",
        "experiment.add_model_version(model=\"Classifier\", algorithm=\"Random Forest\",\n",
        "                                              hyper_parameters=hyper_parameters, metrics=metrics, attachment=\"Feature_importance.png\")\n",
        "\n",
        "# We complete the current experiment's run \n",
        "## The created model version will be automatically attached as output of the run\n",
        "experiment.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsk--891vDc_"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:09:41.221193Z",
          "iopub.status.busy": "2020-10-15T16:09:41.220253Z",
          "iopub.status.idle": "2020-10-15T16:10:04.232901Z",
          "shell.execute_reply": "2020-10-15T16:10:04.233658Z"
        },
        "id": "2rA5PNaWTITB",
        "papermill": {
          "duration": 23.082833,
          "end_time": "2020-10-15T16:10:04.233832",
          "exception": false,
          "start_time": "2020-10-15T16:09:41.150999",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# we declare the dataset versions and code to use as inputs of our run\n",
        "experiment.start(inputs=[input_code, train_ds_version, test_ds_version],\n",
        "                run_properties={\"Property1\": \"Value 1\", \"property2\": \"Value 2\"})\n",
        "\n",
        "n_estimators=300\n",
        "max_depth=16\n",
        "learning_rate=0.1\n",
        "random_state=42\n",
        "\n",
        "xgb_clf = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, random_state=random_state)\n",
        "xgb_clf.fit(X_train_transformed, y_train)\n",
        "\n",
        "train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(xgb_clf)\n",
        "hyper_parameters = {\"n_estimators\": n_estimators, \"max_depth\": max_depth, \"learning_rate\": learning_rate,\"random_state\": random_state}\n",
        "metrics = {\"Train_mcc\":  round(train_mcc, 3),\"test_mcc\":  round(test_mcc, 3), \"train_roc_auc\":  round(train_roc_auc, 3), 'test_roc_auc':  round(test_roc_auc, 3)}\n",
        "\n",
        "plot_feature_importance(feature_columns, xgb_clf.feature_importances_, top_n_features=15)\n",
        "\n",
        "# Let's log the model we trained along with its metrics, as a new version \n",
        "# of the \"Classifier\" model in Vectice.\n",
        "experiment.add_model_version(model=\"Classifier\", algorithm=\"XGBoost\", hyper_parameters=hyper_parameters, metrics=metrics, attachment=\"Feature_importance.png\")\n",
        "\n",
        "# We complete the current experiment's run \n",
        "## The created model version will be automatically attached as output of the run\n",
        "experiment.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLcFQi0ReTIp"
      },
      "source": [
        "### Catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg-e6bg_eQfi"
      },
      "outputs": [],
      "source": [
        "!pip install -q catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-15T16:13:00.960791Z",
          "iopub.status.busy": "2020-10-15T16:13:00.959979Z",
          "iopub.status.idle": "2020-10-15T16:13:24.132024Z",
          "shell.execute_reply": "2020-10-15T16:13:24.130542Z"
        },
        "id": "o1KYFAghTITC",
        "papermill": {
          "duration": 23.246009,
          "end_time": "2020-10-15T16:13:24.132197",
          "exception": false,
          "start_time": "2020-10-15T16:13:00.886188",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# we declare the dataset versions and code to use as inputs of our run\n",
        "experiment.start(inputs=[input_code, train_ds_version, test_ds_version],\n",
        "                run_properties={\"Property1\": \"Value 1\", \"property2\": \"Value 2\"})\n",
        "\n",
        "loss_function='Logloss'\n",
        "iterations=500\n",
        "depth=6\n",
        "eval_metric='AUC'\n",
        "l2_leaf_reg=1\n",
        "random_state=42\n",
        "verbose=0\n",
        "cat_features=list(range(X_cb_train_transformed.shape[1]))\n",
        "catboost_clf = CatBoostClassifier(loss_function=loss_function, iterations=iterations, depth=depth, l2_leaf_reg=l2_leaf_reg, \n",
        "                                  cat_features=cat_features, \n",
        "                                  eval_metric=eval_metric, random_state=random_state, verbose=verbose)\n",
        "catboost_clf.fit(X_cb_train_transformed, y_train)\n",
        "\n",
        "train_mcc, test_mcc, train_roc_auc, test_roc_auc = performance_measures(catboost_clf, X_tr=X_cb_train_transformed, X_ts=X_cb_test_transformed)\n",
        "\n",
        "metrics = {\"Train_mcc\":  round(train_mcc, 3),\"test_mcc\":  round(test_mcc, 3), \"train_roc_auc\":  round(train_roc_auc, 3), 'test_roc_auc':  round(test_roc_auc, 3)}\n",
        "hyper_parameters = {\"loss_function\": loss_function), \"iterations\": iterations, \"categorical features\": list(range((X_cb_train_transformed.shape[1]))),\n",
        "              \"verbose\": verbose, \"depth\": depth, \"eval_metric\": eval_metric, \"l2_leaf_reg\": l2_leaf_reg, \"random_state\": random_state}\n",
        "plot_feature_importance(feature_columns, catboost_clf.feature_importances_, top_n_features=15)\n",
        "\n",
        "# Let's log the model we trained along with its metrics, as a new version \n",
        "# of the \"Classifier\" model in Vectice.\n",
        "experiment.add_model_version(model=\"Classifier\", algorithm=\"CatBoost\", hyper_parameters=hyper_parameters, metrics=metrics, attachment=\"Feature_importance.png\")\n",
        "# We complete the current experiment's run \n",
        "## The created model version will be automatically attached as output of the run\n",
        "experiment.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rP0a61MaV0V"
      },
      "source": [
        "We can update a model's type or description by using experiment.update_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09SbcDLtaV0W"
      },
      "outputs": [],
      "source": [
        "experiment.update_model(model=\"Classifier\", type=ModelType.CLASSIFICATION, description=\"Model description\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NXG9HPhTITE",
        "papermill": {
          "duration": 0.07176,
          "end_time": "2020-10-15T16:26:58.887186",
          "exception": false,
          "start_time": "2020-10-15T16:26:58.815426",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSqi4tuZRA07"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "# We can save those plots and add them to the model version by using \n",
        "## experiment.add_model_version_attachment(file=\"File name\", model_version= \"The model version name or id\", model=\"The model name or id\")\n",
        "plot_custom_roc_curve('Logistic Regression', y_test, logistic_reg.decision_function(X_test_transformed))\n",
        "plot_custom_roc_curve('Random Forest', y_test, forest_clf.predict_proba(X_test_transformed)[:,1])\n",
        "plot_custom_roc_curve('XGBoost', y_test, xgb_clf.predict_proba(X_test_transformed)[:,1])\n",
        "plot_custom_roc_curve('CatBoost', y_test, catboost_clf.predict_proba(X_cb_test_transformed)[:,1])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xklmP-FXTISq",
        "RKL7m-3hskLx"
      ],
      "name": "Amazon_employee_access_challenge.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.13 ('aml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "papermill": {
      "duration": 1154.318387,
      "end_time": "2020-10-15T16:27:25.821888",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-10-15T16:08:11.503501",
      "version": "2.1.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f3244003ce06b8d4815ef5829ab9017a925cc64cacb0a80d02b1f6e20420f2b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
