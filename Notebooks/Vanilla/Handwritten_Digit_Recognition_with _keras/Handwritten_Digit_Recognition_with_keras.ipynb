{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Handwritten_Digit_Recognition_with_keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/vectice/vectice-examples/blob/master/Notebooks/Vanilla/Handwritten_Digit_Recognition_with%20_keras/Handwritten_Digit_Recognition_with_keras_vectice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handwritten digits recognition \n",
        "\n"
      ],
      "metadata": {
        "id": "1mRYJHN19aar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What this is about?\n",
        "The handwritten digit recognition is the ability of computers to recognize human handwritten digits. Handwritten digits are not perfectly written which makes it hard for machine to recognize them. Handwritten digits recognition could be a solution for several problems, such as those which use the image of a digit and recognizes the digit present in the image, for example."
      ],
      "metadata": {
        "id": "bsA4pNFZ9gvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install vectice "
      ],
      "metadata": {
        "id": "PH52e51crcqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectice provides a generic metadata layer that is potentially suitable for most data science workflows.\n",
        "\n",
        "For this tutorial we will use the keras library for modeling and track experiments directly through our Python SDK to illustrate how to fine-tune exactly what you would like to track: metrics, etc. The same mechanisms would apply to R, Java or even more generic REST APIs to track metadata from any programming language and library."
      ],
      "metadata": {
        "id": "6uOw7EUDBONh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a link to the Python SDK Documentation, it's not final nor complete and it is updated as we go along. \n",
        "[Python SDK Documentation](https://doc-dev.vectice.com/)"
      ],
      "metadata": {
        "id": "5MRj2eWBq38E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip3 install -q vectice"
      ],
      "outputs": [],
      "metadata": {
        "id": "ko8fphPjZI3x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip show vectice"
      ],
      "outputs": [],
      "metadata": {
        "id": "obfp9uQ9k1CK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the necessary libraries for this project"
      ],
      "metadata": {
        "id": "rznOZr-Hr8Ek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip3 install -q tensorflow\r\n",
        "!pip3 install -q keras\r\n",
        "!pip3 install -q pillow\r\n",
        "!pip3 install -q numpy"
      ],
      "outputs": [],
      "metadata": {
        "id": "_3m2n6DqZ6Ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the libraries and the dataset"
      ],
      "metadata": {
        "id": "6zcRODIwalUC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# organize imports\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers.core import Dense, Activation, Dropout\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.utils import np_utils\r\n",
        "# fix a random seed for reproducibility\r\n",
        "np.random.seed(9)"
      ],
      "outputs": [],
      "metadata": {
        "id": "SkOc3DKqahaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User inputs for the model"
      ],
      "metadata": {
        "id": "GxOa4vmKscMO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# user inputs\r\n",
        "nb_epoch = 25\r\n",
        "num_classes = 10\r\n",
        "batch_size = 128\r\n",
        "train_size = 60000\r\n",
        "test_size = 10000\r\n",
        "v_length = 784\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "RzvbWiLNg4iX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **nb_epoch**: Number of epochs. An epoch is an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.\n",
        "- **num_classes**: Digits classes (from 0 to 9)\n",
        "- **bacth_size**: It defines the number of samples that will be propagated through the network. For exampe, if we have 1000 training samples, with a batch_size of 100 the model will train on the first 100 training samples then, the second 100 training sample on so on to the end of the training set. \n",
        "- **train_size**: Number of images in the training set\n",
        "- **test_size**: Number of images in the test set\n",
        "- **v_length**: Dimension of flattened input image size i.e. if input image size is [28x28], then v_length = 784"
      ],
      "metadata": {
        "id": "WcR0FVdqYN_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process the data"
      ],
      "metadata": {
        "id": "4XPvr10FshxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial we will use the MNIST keras dataset. The MNIST dataset is a low-complexity data collection of handwritten digits containing 70,000 28x28 black and white images representing the digits zero through nine. It's used to train and test various supervised machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "d3X8O7nJD86L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# split the mnist data into train and test\r\n",
        "(trainData, trainLabels), (testData, testLabels) = mnist.load_data()\r\n",
        "print(\"[INFO] train data shape: {}\".format(trainData.shape))\r\n",
        "print(\"[INFO] test data shape: {}\".format(testData.shape))\r\n",
        "print(\"[INFO] train samples: {}\".format(trainData.shape[0]))\r\n",
        "print(\"[INFO] test samples: {}\".format(testData.shape[0]))"
      ],
      "outputs": [],
      "metadata": {
        "id": "mhh--Sbng-6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "trainData contains the data from which our neural network (model) will learn to recognize images and trainLabels contains the labels (what's in the picture). testData will be used to check if the model can correctly recognize a digit that it hasn't sees before. To evaluate this we will use DataLabels which contains labels for the test set"
      ],
      "metadata": {
        "id": "MixPKAJuEXA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshape and normalize the data\n",
        "**Reshaping**\n",
        "\n",
        "If we process an image in a neural network, it expects a vector and not a two-dimensional array (unless we use a convolution). Therefore, before further processing, we should convert the training set to 60,000 x 784 (28 * 28). To change the shape of the data we will use the reshape function.\n",
        "\n",
        "**Normalization**\n",
        "\n",
        "The value of each pixel – indicating the gray level of a given pixel – should be between 0 (typically 0 is completely black) and 255. We divide our data by 255 to make sure that it's between 0 and 1."
      ],
      "metadata": {
        "id": "kjJUuFXTJovY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# reshape the dataset\r\n",
        "trainData = trainData.reshape(train_size, v_length)\r\n",
        "testData = testData.reshape(test_size, v_length)\r\n",
        "trainData = trainData.astype(\"float32\")\r\n",
        "testData = testData.astype(\"float32\")\r\n",
        "trainData /= 255\r\n",
        "testData /= 255\r\n",
        "\r\n",
        "print(\"[INFO] train data shape: {}\".format(trainData.shape))\r\n",
        "print(\"[INFO] test data shape: {}\".format(testData.shape))\r\n",
        "print(\"[INFO] train samples: {}\".format(trainData.shape[0]))\r\n",
        "print(\"[INFO] test samples: {}\".format(testData.shape[0]))"
      ],
      "outputs": [],
      "metadata": {
        "id": "SRjnegTpg-9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the process, we want the neural network to compare two values and then calculate how far is it from the target. This cannot be done without transforming the target (labels) into so-called one-hot encoding. As a result, our “1” will be written in the following form:\n",
        "\n",
        "[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "If we wanted to write the value 5 in this way, the vector would look like this:\n",
        "\n",
        "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "This operation of converting a scalar to a vector is performed by the function to_categorical().\n",
        "\n"
      ],
      "metadata": {
        "id": "co65DL6ELXqK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# convert class vectors to binary class matrices --> one-hot encoding\r\n",
        "mTrainLabels = np_utils.to_categorical(trainLabels, num_classes)\r\n",
        "mTestLabels = np_utils.to_categorical(testLabels, num_classes)"
      ],
      "outputs": [],
      "metadata": {
        "id": "cviXB1Lwg_B6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to your Vectice workspace and your Vectice project"
      ],
      "metadata": {
        "id": "IPjC48IKsm-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to need an API token and a project token. An API token is used to secure requests between your existing tools and Vectice. You can create and manage those at the API Tokens tab in your workspace, and they impersonate you and your rights per workspace, so we strongly recommend you to avoid sharing them. A project token is used to target the project you're working on in the UI and can found (after creating a project) in the Project settings page, and anyone working on the project can see it and copy/paste it."
      ],
      "metadata": {
        "id": "mzLA-bl3lWcC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# In order to use Vectice SDK, let's set up the configurations first.\r\n",
        "# The Vectice API key below can be generated from the UI.\r\n",
        "# For better security, the settings can also be put into a dedicated file called `.vectice` or `.env` if you're working locally.\r\n",
        "from vectice import Vectice\r\n",
        "from vectice.models import JobType\r\n",
        "from vectice.entity.model import ModelType\r\n",
        "\r\n",
        "os.environ['VECTICE_API_ENDPOINT']= \"beta.vectice.com\"\r\n",
        "##Complete with your Vectice API token\r\n",
        "os.environ['VECTICE_API_TOKEN'] = \"\"\r\n",
        "\r\n",
        "## Complete with your project token\r\n",
        "vectice = Vectice(project_token=\"\")\r\n",
        "print(vectice)"
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "I_9_EQuBmAJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dataset version (empty in this case since we download our data directly from here https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz) containing your data to use them as input for your models. That can be done through the UI by going to your project, clicking on datasets and then click on add (you should add a connection to be able to create a dataset)\n",
        "\n",
        "Create a dataset version based on the dataset you created above"
      ],
      "metadata": {
        "id": "5jqHGBF3mvqr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "##We use autoversioning here\r\n",
        "input_ds_version = vectice.create_dataset_version().with_parent_name(\"dataset_name\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "iqKKyOT9m3kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get different user versions\n",
        "Generate a random user version by calling get_random_string"
      ],
      "metadata": {
        "id": "xmvdKQCb8MIe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Let's generate some unique names for our following modeling experiments\r\n",
        "import random\r\n",
        "import string\r\n",
        "def get_random_string(length):\r\n",
        "    return \"\".join(random.choice(string.ascii_letters) for i in range(length))\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "1_ZvAbOU8Zhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation"
      ],
      "metadata": {
        "id": "cDOJwvVEsqRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using a [sequential model](https://keras.io/getting-started/sequential-model-guide/) (a Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor) and two types of layers: Dense and Dropout.\n",
        "Dense is a standard layer of the neural network in which each neuron is connected to each neuron of the next layer. Dropout is a layer that prevents the phenomenon of over-fitting, i.e. over-matching the operation of the neural network to training data. \n",
        "- We are using 3 Dense layers and 2 Dropout layers.\n",
        "-  Each layer has an activation function. Activation functions available in Keras are listed here: https://keras.io/activations/\n",
        "- Each layer as the first argument takes the size of the output vector for the layer (the first layer accepts 784 parameters and outputs 512)\n",
        "- We use Dropout in our neural network to save it from overfitting. To prevent overfitting, it randomly chooses a fraction of units and set to 0 at each update\n"
      ],
      "metadata": {
        "id": "YR-wx9PtMPOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code creates a DNN (Dense Neural Network) model. Complete the code by creating and starting a job run."
      ],
      "metadata": {
        "id": "_g2Ujqy3nbD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# create the model\r\n",
        "uri = \"https://github.com/vectice/vectice-examples\"\r\n",
        "script_relative_path=\"Notebooks/Vanilla/Handwritten_Digit_Recognition_with%20_keras/Handwritten_Digit_Recognition_with_keras_vectice.ipynb\"\r\n",
        "input_code = Vectice.create_code_version_with_github_uri(uri=uri, script_relative_path=script_relative_path)\r\n",
        "\r\n",
        "vectice.create_run(job_name = \"Train model with keras\", job_type = JobType.TRAINING)\r\n",
        "\r\n",
        "vectice.start_run(inputs=[input_ds_version, input_code])\r\n",
        "\r\n",
        "model = Sequential(name=\"Digits_recognition_sequential\")\r\n",
        "model.add(Dense(512, input_shape=(784,)))\r\n",
        "model.add(Activation(\"relu\"))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(256))\r\n",
        "model.add(Activation(\"relu\"))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(num_classes))\r\n",
        "model.add(Activation(\"softmax\"))\r\n",
        "\r\n",
        "# summarize the model\r\n",
        "model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "ROtxIPt7n1mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define a list containing information about each layer in our neural network"
      ],
      "metadata": {
        "id": "upVZTZT5oJa4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "layers_list = []\r\n",
        "for layer in model.layers:\r\n",
        "  layers_list+=[(\"Layer name\", str(layer.name)+\" layer\"), (str(layer.name)+\" layer \"+\" number of input parameters\", str(layer.input_shape[1])),(str(layer.name)+\" layer \"+\" number of output parameters\",str(layer.output_shape[1]))]\r\n",
        "layers_list"
      ],
      "outputs": [],
      "metadata": {
        "id": "2N4jewVK1Tm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the model, we have to compile it. To do this, we call the method compile specifying:\n",
        "- Type of optimizer: https://keras.io/optimizers/\n",
        "- Loss function: https://keras.io/losses/\n",
        "- Optional metrics that we want for our model to register (accuracy for example)"
      ],
      "metadata": {
        "id": "Vb60dwIbOHqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# compile the model\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "MsvrDVHlhLAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the training of the neural network by using the fit method.\n",
        "\n",
        "You can try to play with the network – change the number of epochs, batch_size, type of optimizer or the structure of the network itself."
      ],
      "metadata": {
        "id": "pKWRdWwpOeSr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# fit the model\r\n",
        "history = model.fit(trainData, mTrainLabels, validation_data=(testData, mTestLabels), batch_size=batch_size, epochs=nb_epoch, verbose=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zGIZ0XC2hLC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation"
      ],
      "metadata": {
        "id": "XToHWjdpsuUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we calculate some metrics in order to evaluate the model.\n",
        "We already created a job run aboce and started it. Please complete the code by creating a model version, adding the metrics and the parameters (properties) to it and end the run."
      ],
      "metadata": {
        "id": "DVYVBG83oUyf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# print the history keys\r\n",
        "print(history.history.keys())\r\n",
        "# evaluate the model\r\n",
        "scores = model.evaluate(testData, mTestLabels, verbose=0)\r\n",
        "\r\n",
        "# history plot for accuracy\r\n",
        "plt.plot(history.history[\"accuracy\"])\r\n",
        "plt.plot(history.history[\"val_accuracy\"])\r\n",
        "plt.title(\"Model Accuracy\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Accuracy\")\r\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# history plot for accuracy\r\n",
        "plt.plot(history.history[\"loss\"])\r\n",
        "plt.plot(history.history[\"val_loss\"])\r\n",
        "plt.title(\"Model Loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# print the results\r\n",
        "print(\"[INFO] test score - {}\".format(scores[0]))\r\n",
        "print(\"[INFO] test accuracy - {}\".format(scores[1]))\r\n",
        "\r\n",
        "\r\n",
        "metrics = [('test score',scores[0]), (\"test accuracy\",scores[1])]\r\n",
        "\r\n",
        "properties = [(\"Algorithm\",\"Neural networks\"), (\"Model\", \"Sequential (keras)\")] +layers_list+ [(\"Number of epochs\",str(nb_epoch)), (\"number of digit classes (from 0 to 9) \", str(num_classes)), (\"Batch size \", str(batch_size)), (\"Number of images in the training set \", str(train_size)), (\"Number of images in the test set \", str(test_size)), (\"Dimension of flattened input image size\", str(v_length))]\r\n",
        "model_version1 = vectice.create_model_version().with_parent_name(\"Classification\").with_algorithm(\"Neural network\").with_type(ModelType.CLASSIFICATION).with_properties(properties).with_metrics(metrics).with_user_version(get_random_string(12))\r\n",
        "\r\n",
        "\r\n",
        "vectice.end_run(outputs=[model_version1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "507txQ-Wov6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's got an accuracy of 0.9826 on the test set. The difference between accuracy on the training and test set is only 0.01% and seems great"
      ],
      "metadata": {
        "id": "QrrMF9lIQomF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use our testData to test the model on picture it hasn't seen before. We can do so by using model.predection in keras"
      ],
      "metadata": {
        "id": "UaVP2yNNS8Q2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "predictions = model.predict(testData[0:100])"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ldn0rYMeQaWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method returns a 100-element scoreboard. Each element indicates the probabilities that the input belongs to a given class."
      ],
      "metadata": {
        "id": "6kEppwsiTaLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "predictions[0]"
      ],
      "outputs": [],
      "metadata": {
        "id": "42yVrS_6Rln1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see in this example the probabilities are very low except for the eight element (class 7, counted from 0) wich has a probability of 1. So the model predicted that it's 7. Let's verify that"
      ],
      "metadata": {
        "id": "RAUz8f-wTgaR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "np.argmax(predictions[0])"
      ],
      "outputs": [],
      "metadata": {
        "id": "7z4iqIFtg_FA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.imshow(testData[0].reshape(28,28))"
      ],
      "outputs": [],
      "metadata": {
        "id": "iNUwHh2JRtRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the number is 7"
      ],
      "metadata": {
        "id": "EWMa_oBxUErL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a look at the predictions (after treating the probabilities with the argmax function and specifying to the function in what dimension it should analyze data (in our case, along the y axis, i.e. axis = 1) because we are using a two-dimensional array)"
      ],
      "metadata": {
        "id": "B6kypNDZUJx4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "np.argmax(predictions, axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "NAOWWHDfR0ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the labels for the data on which the model did the predictions and compare"
      ],
      "metadata": {
        "id": "fkjtxaoNUJLg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "testLabels[0:100]"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q3zPtqWlR1TI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "np.argmax(predictions, axis=1) == testLabels[0:100]"
      ],
      "outputs": [],
      "metadata": {
        "id": "bDzpUIIyST4r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "np.mean(np.argmax(predictions, axis=1) == testLabels[0:100])\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "tjUaQJEESZQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predictions coincide with reality"
      ],
      "metadata": {
        "id": "7OEr9xp0WYPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also verify if there are wrong predictions by running this cell which uses Numpy's argmin function"
      ],
      "metadata": {
        "id": "pRAvWenJU-0V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "wrong_pred = np.argmin(np.argmax(predictions, axis=1) == testLabels[0:100])\n",
        "wrong_pred"
      ],
      "outputs": [],
      "metadata": {
        "id": "BD-RQuW8SitL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No element was wrongly predicted (for example, if wrong_pred=2 we can say that the the element in the position 2 is wrongly predicted)"
      ],
      "metadata": {
        "id": "yOCc44-MVg5Q"
      }
    }
  ]
}