{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectice/vectice-examples/blob/master/Notebooks/Vanilla/Customer_satisfaction_example/customer_satisfaction_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Santander customer satisfaction challenge"
   ],
   "metadata": {
    "id": "c65d22b9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem"
   ],
   "metadata": {
    "id": "cd618113"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Customer satisfaction is a key measure of success for all businesses. Unhappy customers don't stay with the same provider and they rarely voice their dissatisfaction before leaving. In this context, Santander bank launched a challenge in Kaggle in order to build models that predict potential unhappy customers\r\n",
    "\r\n",
    "---\r\n",
    "\r\n"
   ],
   "metadata": {
    "id": "13c53800"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Objective"
   ],
   "metadata": {
    "id": "eb29db7c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The objective of this competition is to be able to identify unhappy customers early and anticipate their leaving which would allow the company to take proactive steps to improve a customer's happiness before it's too late. In this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience."
   ],
   "metadata": {
    "id": "ba76d94d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {
    "id": "26a9a7e8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is an anonymized dataset containing a large number of numeric variables. The \"TARGET\" column is the variable to predict. It equals 1 for unsatisfied customers and 0 for satisfied customers. The task is to predict the probability that each customer in the test set is an unsatisfied customer.\r\n",
    "- train.csv: (371 columns): The training set including the target\r\n",
    "- test.csv: (370 columns): The test set without the target"
   ],
   "metadata": {
    "id": "6a0c7084"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Install Vectice and GCS packages\n"
   ],
   "metadata": {
    "id": "4fc51dd4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vectice provides a generic metadata layer that is potentially suitable for most data science workflows. For this tutorial we will use the sickit-learn library for modeling and track experiments directly through our Python SDK to illustrate how to fine-tune exactly what you would like to track: metrics, etc. The same mechanisms would apply to R, Java or even more generic REST APIs to track metadata from any programming language and library."
   ],
   "metadata": {
    "id": "4c3ccd71"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is a link to the Python SDK Documentation, it's not final nor complete and it is updated as we go along. \n",
    "[Python SDK Documentation](https://doc-dev.vectice.com/)"
   ],
   "metadata": {
    "id": "g0tZOjR0q06E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip3 install -q fsspec\n",
    "!pip3 install -q gcsfs\n",
    "!pip3 install -q vectice"
   ],
   "outputs": [],
   "metadata": {
    "id": "a02e4135"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip3 show vectice"
   ],
   "outputs": [],
   "metadata": {
    "id": "7d9d12a9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install the required packages"
   ],
   "metadata": {
    "id": "21d4a5d8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Especially if you're working locally and you didn't already install them**"
   ],
   "metadata": {
    "id": "FvmOFTScUPNV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install -q numpy\r\n",
    "!pip install -q pandas\r\n",
    "!pip install -q matplotlib\r\n",
    "!pip install -q seaborn\r\n",
    "!pip install -q sklearn\r\n",
    "!pip install -q lightgbm\r\n",
    "!pip install -q imblearn"
   ],
   "outputs": [],
   "metadata": {
    "id": "cf34495c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import the required packages\r\n"
   ],
   "metadata": {
    "id": "d9b465b6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import numpy as np # linear algebra\r\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from vectice.models import JobType\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, auc\r\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\r\n",
    "import lightgbm as lgb\r\n",
    "from lightgbm import plot_importance\r\n",
    "from imblearn.over_sampling import SMOTE\r\n",
    "from collections import Counter\r\n",
    "\r\n",
    "plt.style.use('seaborn')\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {
    "id": "0e2bc921"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retreive the data from GS"
   ],
   "metadata": {
    "id": "7e63a557"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you're not using Google Colab, don't run the following cell and put the json key access file that was provided with your tutorial account in the same directory with this notebook"
   ],
   "metadata": {
    "id": "-JyVpzG_dM-H"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Don't run this cell if you're not using Colab\r\n",
    "# Load your json key file to access GCS that was provided with your tutorial account \r\n",
    "# The name should be something like test.json\r\n",
    "from google.colab import files\r\n",
    "uploaded = files.upload()"
   ],
   "outputs": [],
   "metadata": {
    "id": "9v2HQ-oI9BWu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Once your file is loaded set the credentials for GCS and load the file\r\n",
    "# in a pandas frame, double check the json file name you uploaded below.\r\n",
    "\r\n",
    "### Complete with the name of the JSON key file to access GCS. It can be found in the tutorial page\r\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'Name of the GCS key access file (readerKey.json)'\r\n",
    "# The original source dataset is already declared in the Vectice UI as \"customer_satisfaction_train\"\r\n",
    "# and its connection to \"gs://vectice-examples-samples/Customer_satisfaction_challenge/\" has been established\r\n",
    "train_df = pd.read_csv(\"gs://vectice-examples-samples/Customer_satisfaction_challenge/dataset.csv\")\r\n",
    "# Run head to make sure the data was loaded properly\r\n",
    "print(train_df.head())"
   ],
   "outputs": [],
   "metadata": {
    "id": "1b53c2b1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data exploration\r\n",
    "\r\n",
    "Data exploration enables us to take a first look on the data, can enhance the overall understanding of the characteristics of the data domain and helps to detect correlation between the features, thereby allowing for the creation of more accurate models"
   ],
   "metadata": {
    "id": "e786cc41"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Train Data Shape : \",train_df.shape)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:08.505604Z",
     "iopub.status.busy": "2021-04-10T09:30:08.504426Z",
     "iopub.status.idle": "2021-04-10T09:30:08.507867Z",
     "shell.execute_reply": "2021-04-10T09:30:08.506112Z"
    },
    "id": "92aff10e",
    "papermill": {
     "duration": 0.04008,
     "end_time": "2021-04-10T09:30:08.507985",
     "exception": false,
     "start_time": "2021-04-10T09:30:08.467905",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df['TARGET'].value_counts()\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "b5bebe78"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.info()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:08.595439Z",
     "iopub.status.busy": "2021-04-10T09:30:08.594699Z",
     "iopub.status.idle": "2021-04-10T09:30:08.599672Z",
     "shell.execute_reply": "2021-04-10T09:30:08.599131Z"
    },
    "id": "f82dd09e",
    "papermill": {
     "duration": 0.061924,
     "end_time": "2021-04-10T09:30:08.599807",
     "exception": false,
     "start_time": "2021-04-10T09:30:08.537883",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.describe()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:08.740556Z",
     "iopub.status.busy": "2021-04-10T09:30:08.739623Z",
     "iopub.status.idle": "2021-04-10T09:30:10.278376Z",
     "shell.execute_reply": "2021-04-10T09:30:10.278859Z"
    },
    "id": "913fa320",
    "papermill": {
     "duration": 1.646631,
     "end_time": "2021-04-10T09:30:10.279041",
     "exception": false,
     "start_time": "2021-04-10T09:30:08.632410",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features = train_df.drop(['ID','TARGET'],axis=1)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:10.424731Z",
     "iopub.status.busy": "2021-04-10T09:30:10.422820Z",
     "iopub.status.idle": "2021-04-10T09:30:10.425415Z",
     "shell.execute_reply": "2021-04-10T09:30:10.425819Z"
    },
    "id": "f5f1ad5c",
    "papermill": {
     "duration": 0.113231,
     "end_time": "2021-04-10T09:30:10.426014",
     "exception": false,
     "start_time": "2021-04-10T09:30:10.312783",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory data analysis (EDA)\r\n",
    "* Target Percent\r\n",
    "* Check Multicollinearity\r\n",
    "* Check Outlier\r\n",
    "\r\n",
    "EDA is a technique that helps to explore and understand the data sets by summarizing their main characteristics often plotting them visually. It consists of Histograms, Box plot, Scatter plot and many more. EDA is about gathering as many insights from data as we can in order to understand it"
   ],
   "metadata": {
    "id": "cc8c3198",
    "papermill": {
     "duration": 0.033393,
     "end_time": "2021-04-10T09:30:10.493222",
     "exception": false,
     "start_time": "2021-04-10T09:30:10.459829",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.DataFrame(train_df['TARGET'].value_counts())"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:10.564726Z",
     "iopub.status.busy": "2021-04-10T09:30:10.563609Z",
     "iopub.status.idle": "2021-04-10T09:30:10.574704Z",
     "shell.execute_reply": "2021-04-10T09:30:10.574163Z"
    },
    "id": "02d9dde2",
    "papermill": {
     "duration": 0.048701,
     "end_time": "2021-04-10T09:30:10.574875",
     "exception": false,
     "start_time": "2021-04-10T09:30:10.526174",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training set is way imbalanced (73012 zeros vs 3008 ones), so some algorithms may learn mostly from the 0 which can affect our predictions. We address that by using oversampling\r\n"
   ],
   "metadata": {
    "id": "f968764e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f, ax = plt.subplots(1,2,figsize=(10,4))\r\n",
    "train_df['TARGET'].value_counts().plot.pie(\r\n",
    "    explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True\r\n",
    ")\r\n",
    "sns.countplot('TARGET', data=train_df, ax=ax[1])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:10.661950Z",
     "iopub.status.busy": "2021-04-10T09:30:10.650205Z",
     "iopub.status.idle": "2021-04-10T09:30:10.960179Z",
     "shell.execute_reply": "2021-04-10T09:30:10.959645Z"
    },
    "id": "9c79ec39",
    "papermill": {
     "duration": 0.351011,
     "end_time": "2021-04-10T09:30:10.960347",
     "exception": false,
     "start_time": "2021-04-10T09:30:10.609336",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "null_value = train_df.isnull().sum().sort_values(ascending=False)\r\n",
    "null_percent = round(train_df.isnull().sum().sort_values(ascending=False)/len(train_df)*100,2)\r\n",
    "pd.concat([null_value, null_percent], axis=1, keys=['Null values', 'Percent'])"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:11.038954Z",
     "iopub.status.busy": "2021-04-10T09:30:11.037745Z",
     "iopub.status.idle": "2021-04-10T09:30:11.169433Z",
     "shell.execute_reply": "2021-04-10T09:30:11.169937Z"
    },
    "id": "28ef4f58",
    "papermill": {
     "duration": 0.17417,
     "end_time": "2021-04-10T09:30:11.170170",
     "exception": false,
     "start_time": "2021-04-10T09:30:10.996000",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ther is no column with null values"
   ],
   "metadata": {
    "id": "6618c7cd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Correlation**\r\n",
    "\r\n",
    "If we have a big correlation, we have a problem of multicolinearity. That means that there are some features that depend of other features, so we should reduce the dimentionality of our data (if A depends of B, we should either find a way to aggregate or combine the two features and turn it into one variable or drop one of the variables that are too highly correlated with another) and that can be adressed using Principal component analysis (PCA)"
   ],
   "metadata": {
    "id": "fcde4213"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features[features.columns[:8]].corr()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:11.257537Z",
     "iopub.status.busy": "2021-04-10T09:30:11.256841Z",
     "iopub.status.idle": "2021-04-10T09:30:11.287243Z",
     "shell.execute_reply": "2021-04-10T09:30:11.287732Z"
    },
    "id": "50606c02",
    "papermill": {
     "duration": 0.07645,
     "end_time": "2021-04-10T09:30:11.287892",
     "exception": false,
     "start_time": "2021-04-10T09:30:11.211442",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.heatmap(features[features.columns[:8]].corr(),annot=True,cmap='YlGnBu')\r\n",
    "fig=plt.gcf()\r\n",
    "fig.set_size_inches(10,8)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:11.374340Z",
     "iopub.status.busy": "2021-04-10T09:30:11.373115Z",
     "iopub.status.idle": "2021-04-10T09:30:12.063179Z",
     "shell.execute_reply": "2021-04-10T09:30:12.063689Z"
    },
    "id": "0547ae73",
    "papermill": {
     "duration": 0.736357,
     "end_time": "2021-04-10T09:30:12.063872",
     "exception": false,
     "start_time": "2021-04-10T09:30:11.327515",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "=> We Can Check Multicollinearity\r\n",
    "\r\n",
    "Multicollinearity is a phenomenon in which one independent variable is highly correlated with one or more of the other independent variables"
   ],
   "metadata": {
    "id": "e9917207",
    "papermill": {
     "duration": 0.036253,
     "end_time": "2021-04-10T09:30:12.138938",
     "exception": false,
     "start_time": "2021-04-10T09:30:12.102685",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(16,6))\r\n",
    "plt.title(\"Distribution of mean values per row in the train and test set\")\r\n",
    "sns.distplot(train_df[features.columns].mean(axis=1),color=\"black\", kde=True,bins=120, label='train')\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:12.234856Z",
     "iopub.status.busy": "2021-04-10T09:30:12.228916Z",
     "iopub.status.idle": "2021-04-10T09:30:14.659466Z",
     "shell.execute_reply": "2021-04-10T09:30:14.658480Z"
    },
    "id": "a7727ec3",
    "papermill": {
     "duration": 2.483,
     "end_time": "2021-04-10T09:30:14.659617",
     "exception": false,
     "start_time": "2021-04-10T09:30:12.176617",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(16,6))\r\n",
    "plt.title(\"Distribution of std values per rows in the train and test set\")\r\n",
    "sns.distplot(train_df[features.columns].std(axis=1),color=\"blue\",kde=True,bins=120, label='train')\r\n",
    "plt.legend(); plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:14.762204Z",
     "iopub.status.busy": "2021-04-10T09:30:14.759793Z",
     "iopub.status.idle": "2021-04-10T09:30:17.006201Z",
     "shell.execute_reply": "2021-04-10T09:30:17.006691Z"
    },
    "id": "8d5e6a03",
    "papermill": {
     "duration": 2.308513,
     "end_time": "2021-04-10T09:30:17.006870",
     "exception": false,
     "start_time": "2021-04-10T09:30:14.698357",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t0 = train_df[train_df['TARGET'] == 0]\r\n",
    "t1 = train_df[train_df['TARGET'] == 1]\r\n",
    "plt.figure(figsize=(16,6))\r\n",
    "plt.title(\"Distribution of skew values per row in the train set\")\r\n",
    "sns.distplot(t0[features.columns].skew(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\r\n",
    "sns.distplot(t1[features.columns].skew(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\r\n",
    "plt.legend(); plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:17.098785Z",
     "iopub.status.busy": "2021-04-10T09:30:17.097753Z",
     "iopub.status.idle": "2021-04-10T09:30:19.059778Z",
     "shell.execute_reply": "2021-04-10T09:30:19.059210Z"
    },
    "id": "a75dc43a",
    "papermill": {
     "duration": 2.010971,
     "end_time": "2021-04-10T09:30:19.059925",
     "exception": false,
     "start_time": "2021-04-10T09:30:17.048954",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "=> We Can Check Outlier\r\n",
    "\r\n",
    "An outlier is a value or point that differs substantially from the rest of the data"
   ],
   "metadata": {
    "id": "fc2d0ca7",
    "papermill": {
     "duration": 0.038339,
     "end_time": "2021-04-10T09:30:19.138352",
     "exception": false,
     "start_time": "2021-04-10T09:30:19.100013",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.describe()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:19.292152Z",
     "iopub.status.busy": "2021-04-10T09:30:19.291019Z",
     "iopub.status.idle": "2021-04-10T09:30:20.736357Z",
     "shell.execute_reply": "2021-04-10T09:30:20.735793Z"
    },
    "id": "6fcaa813",
    "papermill": {
     "duration": 1.558729,
     "end_time": "2021-04-10T09:30:20.736523",
     "exception": false,
     "start_time": "2021-04-10T09:30:19.177794",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.boxplot(train_df['var3'])"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:20.846567Z",
     "iopub.status.busy": "2021-04-10T09:30:20.845574Z",
     "iopub.status.idle": "2021-04-10T09:30:20.947011Z",
     "shell.execute_reply": "2021-04-10T09:30:20.947475Z"
    },
    "id": "f3b92157",
    "papermill": {
     "duration": 0.166387,
     "end_time": "2021-04-10T09:30:20.947629",
     "exception": false,
     "start_time": "2021-04-10T09:30:20.781242",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.boxplot(train_df['var38'])"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:21.037116Z",
     "iopub.status.busy": "2021-04-10T09:30:21.036179Z",
     "iopub.status.idle": "2021-04-10T09:30:21.150551Z",
     "shell.execute_reply": "2021-04-10T09:30:21.150989Z"
    },
    "id": "1813ccb0",
    "papermill": {
     "duration": 0.161176,
     "end_time": "2021-04-10T09:30:21.151170",
     "exception": false,
     "start_time": "2021-04-10T09:30:20.989994",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training set:\r\n",
    "- Contains continuous and and catigorized data (we should treate carigorized data cuz 10000>1 if we interpret them as numeric values and not catigorical (example IDs)\r\n",
    "- Contains variables with zero variance or non predictive value\r\n",
    "- Contains fake values (-999999) that were introduced to replace missing data\r\n",
    "- Is way imbalanced"
   ],
   "metadata": {
    "id": "15d5668a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\r\n",
    "Data preprocessing alludes to the method of cleaning and arranging the crude data to make it appropriate for building and preparing AI models. Data preprocessing is a procedure that changes crude data into an instructive and lucid arrangement. Our dataset is imbalanced. We will use oversampling to resolve this problem because if not some algorithms will learn more from the zeros than the ones in our training dataset\r\n",
    "\r\n",
    "\r\n",
    "* Processing Outlier Values"
   ],
   "metadata": {
    "id": "f8637fb5",
    "papermill": {
     "duration": 0.043992,
     "end_time": "2021-04-10T09:30:21.238920",
     "exception": false,
     "start_time": "2021-04-10T09:30:21.194928",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df['var3'].replace(-999999,2,inplace=True)\r\n",
    "train_df.describe()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:21.329714Z",
     "iopub.status.busy": "2021-04-10T09:30:21.328663Z",
     "iopub.status.idle": "2021-04-10T09:30:22.622258Z",
     "shell.execute_reply": "2021-04-10T09:30:22.622990Z"
    },
    "id": "075a5caa",
    "papermill": {
     "duration": 1.341621,
     "end_time": "2021-04-10T09:30:22.623220",
     "exception": false,
     "start_time": "2021-04-10T09:30:21.281599",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connect to your Vectice project"
   ],
   "metadata": {
    "id": "434d729b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are going to need an API token and a project token. An API token is used to secure requests between your existing tools and Vectice. You can create and manage those at the API Tokens tab in your workspace, and they impersonate you and your rights per workspace, so we strongly recommend you to avoid sharing them.\r\n",
    "A project token is used to target the project you're working on in the UI and can found (after creating a project) in the Project settings page, and anyone working on the project can see it and copy/paste it."
   ],
   "metadata": {
    "id": "e562e7a0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# In order to use Vectice SDK, let's set up the configurations first.\r\n",
    "# The Vectice API key below can be generated from the UI.\r\n",
    "# For better security, the settings can also be put into a dedicated file called `.vectice` or `.env`.\r\n",
    "## Make sure that you're using the right endpoint \r\n",
    "os.environ['VECTICE_API_ENDPOINT']= \"\"\r\n",
    "os.environ['VECTICE_API_TOKEN'] = \"\"\r\n",
    "\r\n",
    "## Create a Vetice instance to connect to your project using your project token\r\n",
    "## Hint: Do not forget to import vectice (from vectice import Vectice)\r\n",
    "vectice = Vectice(project_token=\"\")\r\n",
    "\r\n",
    "print(vectice)"
   ],
   "outputs": [],
   "metadata": {
    "id": "gHqZV6Ja54tH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Double click to show the syntax\r\n",
    "os.environ['VECTICE_API_ENDPOINT']= \"beta.vectice.com\"\r\n",
    "##Complete with your Vectice API token\r\n",
    "os.environ['VECTICE_API_TOKEN'] = \"\"\r\n",
    "from vectice import Vectice\r\n",
    "## Complete with your project token\r\n",
    "vectice = Vectice(project_token=\"\")\r\n",
    "print(vectice)"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "id": "cd173333"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\r\n",
    "\r\n",
    "It's about creating new input features from your existing ones and can be seen as a process of addition that helps to improve the mode's performance by :\r\n",
    "- Isolating and highlighting key information, which helps the algorithms \"focus\" on what’s important.\r\n",
    "- You can bring in your own domain expertise.\r\n",
    "- Most importantly, once you understand the \"vocabulary\" of feature engineering, you can bring in other people’s domain expertise!\r\n",
    "\r\n",
    "In this part we will:\r\n",
    "* Split Data to Train / Test \r\n",
    "* Train Data to Standard Scaler\r\n",
    "* Target Data to Oversampling by SMOTE"
   ],
   "metadata": {
    "id": "3e341732",
    "papermill": {
     "duration": 0.043784,
     "end_time": "2021-04-10T09:30:22.714674",
     "exception": false,
     "start_time": "2021-04-10T09:30:22.670890",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.drop('ID',axis=1,inplace=True)"
   ],
   "outputs": [],
   "metadata": {
    "id": "bCpyyNIMSahh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = train_df.drop('TARGET',axis=1)\r\n",
    "y = train_df['TARGET']"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:23.107800Z",
     "iopub.status.busy": "2021-04-10T09:30:23.086344Z",
     "iopub.status.idle": "2021-04-10T09:30:23.111031Z",
     "shell.execute_reply": "2021-04-10T09:30:23.110577Z"
    },
    "id": "901292cd",
    "papermill": {
     "duration": 0.12009,
     "end_time": "2021-04-10T09:30:23.111160",
     "exception": false,
     "start_time": "2021-04-10T09:30:22.991070",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resolving the problem of multicolinearity"
   ],
   "metadata": {
    "id": "bc0672d3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are going to use the \"The Pearson correlation\" method. It is the most common method to use for numerical variables; it assigns a value between − 1 and 1, where 0 is no correlation, 1 is total positive correlation, and − 1 is total negative correlation. This is interpreted as follows: a correlation value of 0.7 between two variables would indicate that a significant and positive relationship exists between the two. A positive correlation signifies that if variable A goes up, then B will also go up, whereas if the value of the correlation is negative, then if A increases, B decreases"
   ],
   "metadata": {
    "id": "c8083d72"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def correlation(dataset, threshold):\r\n",
    "    col_corr = set()  # Set of all the names of correlated columns\r\n",
    "    corr_matrix = dataset.corr()\r\n",
    "    for i in range(len(corr_matrix.columns)):\r\n",
    "        for j in range(i):\r\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\r\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\r\n",
    "                col_corr.add(colname)\r\n",
    "    return col_corr"
   ],
   "outputs": [],
   "metadata": {
    "id": "13a7505c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We consider a threshold of 0.9 to avoid high correlation"
   ],
   "metadata": {
    "id": "8795276c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corr_features = correlation(x, 0.9)\r\n",
    "len(set(corr_features))"
   ],
   "outputs": [],
   "metadata": {
    "id": "7fc7f0c8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = x.drop(corr_features,axis=1)"
   ],
   "outputs": [],
   "metadata": {
    "id": "ae9a9fd0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Standardize data\r\n",
    "\r\n",
    "StandardScaler is an important technique that is mainly performed as a preprocessing step before many machine learning models, in order to standardize the range of functionality of the input dataset. It's used to resize the distribution of values ​​so that the mean of the observed values ​​is 0 and the standard deviation is 1"
   ],
   "metadata": {
    "id": "bde1a4b8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler = StandardScaler().fit(x)\r\n",
    "x_scaler = scaler.transform(x)\r\n",
    "x_scaler_df = pd.DataFrame(x_scaler, columns=x.columns)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:23.209151Z",
     "iopub.status.busy": "2021-04-10T09:30:23.207792Z",
     "iopub.status.idle": "2021-04-10T09:30:24.155479Z",
     "shell.execute_reply": "2021-04-10T09:30:24.154951Z"
    },
    "id": "f3352937",
    "papermill": {
     "duration": 1.001235,
     "end_time": "2021-04-10T09:30:24.155631",
     "exception": false,
     "start_time": "2021-04-10T09:30:23.154396",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Principal component analysis (PCA)** \r\n",
    "\r\n",
    "It's a statistical technique used for data reduction without losing its properties. Using PCA can help identify correlations between data points. PCA creates a visualization of data that minimizes residual variance in the least squares sense and maximizes the variance of the projection coordinates"
   ],
   "metadata": {
    "id": "1858b9f4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pca = PCA(n_components=0.95)\r\n",
    "x_scaler_pca = pca.fit_transform(x_scaler)\r\n",
    "x_scaler_pca_df = pd.DataFrame(x_scaler_pca)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:24.246867Z",
     "iopub.status.busy": "2021-04-10T09:30:24.245829Z",
     "iopub.status.idle": "2021-04-10T09:30:28.726482Z",
     "shell.execute_reply": "2021-04-10T09:30:28.725973Z"
    },
    "id": "58f55578",
    "papermill": {
     "duration": 4.528332,
     "end_time": "2021-04-10T09:30:28.726624",
     "exception": false,
     "start_time": "2021-04-10T09:30:24.198292",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_scaler_pca_df.head()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:28.837090Z",
     "iopub.status.busy": "2021-04-10T09:30:28.836300Z",
     "iopub.status.idle": "2021-04-10T09:30:28.840095Z",
     "shell.execute_reply": "2021-04-10T09:30:28.840581Z"
    },
    "id": "dc6033c6",
    "papermill": {
     "duration": 0.070204,
     "end_time": "2021-04-10T09:30:28.840741",
     "exception": false,
     "start_time": "2021-04-10T09:30:28.770537",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pca.explained_variance_ratio_"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:28.937349Z",
     "iopub.status.busy": "2021-04-10T09:30:28.936342Z",
     "iopub.status.idle": "2021-04-10T09:30:28.940300Z",
     "shell.execute_reply": "2021-04-10T09:30:28.940805Z"
    },
    "id": "527ca72d",
    "papermill": {
     "duration": 0.053612,
     "end_time": "2021-04-10T09:30:28.940973",
     "exception": false,
     "start_time": "2021-04-10T09:30:28.887361",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(x_scaler_pca_df.loc[:, 0], x_scaler_pca_df.loc[:, 1], c=y,  cmap=\"copper_r\")\r\n",
    "plt.axis('off')\r\n",
    "plt.colorbar()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:29.052183Z",
     "iopub.status.busy": "2021-04-10T09:30:29.051277Z",
     "iopub.status.idle": "2021-04-10T09:30:30.526530Z",
     "shell.execute_reply": "2021-04-10T09:30:30.526986Z"
    },
    "id": "8b042a6d",
    "papermill": {
     "duration": 1.540039,
     "end_time": "2021-04-10T09:30:30.527130",
     "exception": false,
     "start_time": "2021-04-10T09:30:28.987091",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "=> We cant use PCA since we can't reduce the dimentionality (The variance is represented by multiple variables and we didn't find a small number of variables that enable to represent a considerable part of the variance)"
   ],
   "metadata": {
    "id": "d171e71a",
    "papermill": {
     "duration": 0.045353,
     "end_time": "2021-04-10T09:30:30.619214",
     "exception": false,
     "start_time": "2021-04-10T09:30:30.573861",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split the data and use oversampling"
   ],
   "metadata": {
    "id": "-3PKP2Ux8bbd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ML algorithms can have a poor performance when dealing with datasets in which one or more classes represent only a small proportion of the overall dataset compared with a dominant class. This problem can be solved by balancing the number of examples between different classes. Here we suggest to you SMOTE (Synthetic Minority Over-sampling Technique) that creates synthetic data based on creating new data points that are mid-way between two near neighbours in any particular class"
   ],
   "metadata": {
    "id": "6e_Luz_U9T51"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a dataset containing your dataset to use it as input for your splitting data job. That can be done through the UI by going to your project, clicking on datasets and then click on add (you should add a connection to be able to create a dataset)\r\n",
    "\r\n",
    "Create a dataset version based on the dataset you created above"
   ],
   "metadata": {
    "id": "Ys5h4Ed99b9d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_ds_version = \"\""
   ],
   "outputs": [],
   "metadata": {
    "id": "KDjrlQ-r2Y2o"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Double click to show the syntax\r\n",
    "# Use auto-versioning here\r\n",
    "input_ds_version =  vectice.create_dataset_version().with_parent_name(\"Your dataset's name\")"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "id": "1c3i0UBn2bOM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code splits the dataset to train and test sets and uses the SMOTE methode for oversampling in order to balance our dataset. Please complete itwith creating PREPARATION job run, start it and then declare train_set and test_set as dataset versions (after creating the datasets in the UI) in order to be able to use them as inputs for the different models"
   ],
   "metadata": {
    "id": "GRHuo2f92kcx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#Split data\r\n",
    "scaler_x_train, scaler_x_test, scaler_y_train, scaler_y_test = train_test_split(x_scaler, y, test_size=0.3)\r\n",
    "#Use SMOTE to oversample the dataset\r\n",
    "x_over, y_over = SMOTE().fit_resample(scaler_x_train,scaler_y_train)\r\n",
    "print(sorted(Counter(y_over).items()))\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "_pdU4vz181at"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#@title Double click to show the answer\r\n",
    "\r\n",
    "uri = \"https://github.com/vectice/vectice-examples\"\r\n",
    "script_relative_path=\"Notebooks/Vanilla/Customer_satisfaction_example/customer_satisfaction_challenge.ipynb\"\r\n",
    "input_code = Vectice.create_code_version_with_github_uri(uri=uri, script_relative_path=script_relative_path)\r\n",
    "\r\n",
    "input_ds_version = input_ds_version\r\n",
    "\r\n",
    "# Start a Vectice run. The job type should be PREPARATION in this case.\r\n",
    "\r\n",
    "vectice.create_run(\"jobSplitData_Customer_Satisfaction\", JobType.PREPARATION)\r\n",
    "with vectice.start_run(inputs=[input_ds_version,input_code]) as run:\r\n",
    "#with vectice.start_run(inputs=[input_ds_version]) as run:\r\n",
    "\r\n",
    "#Split data\r\n",
    "  scaler_x_train, scaler_x_test, scaler_y_train, scaler_y_test = train_test_split(x_scaler, y, test_size=0.3)\r\n",
    "#Use SMOTE to oversample the dataset\r\n",
    "  x_over, y_over = SMOTE().fit_resample(scaler_x_train,scaler_y_train)\r\n",
    "  print(sorted(Counter(y_over).items()))\r\n",
    "\r\n",
    " \r\n",
    "# We commented out the code to persist the training and testing test in GCS,\r\n",
    "# because we already generated it for you, but feel free to uncomment it and execute it.\r\n",
    "# The key you were provided for this tutorial may not have write permissions to GCS.\r\n",
    "# Let us know if you want to be able to write files as well and we can issue you a different key.\r\n",
    "\r\n",
    "## Get training and testing data in dataframes in orderto upload them to GCS\r\n",
    "  #train_set = pd.DataFrame(x_over, columns=x.columns).join(pd.DataFrame(y_over, columns=[\"TARGET\"]))\r\n",
    "  #test_set = pd.DataFrame(scaler_x_test, columns=x.columns).join(pd.DataFrame(scaler_y_test, columns=[\"TARGET\"]))\r\n",
    "  #train_set.to_csv (r'gs://vectice-examples-samples/Customer_satisfaction_challenge/training_data.csv', index = False, header = True)\r\n",
    "  #test_set.to_csv (r'gs://vectice-examples-samples/Customer_satisfaction_challenge/testing_data.csv', index = False, header = True)\r\n",
    "\r\n",
    "  # Don't forget to create the datasets before creating train_ds_version and test_ds_version \r\n",
    "\r\n",
    "  train_ds_version = vectice.create_dataset_version().with_parent_name(\"train dataset name\")\r\n",
    "  test_ds_version = vectice.create_dataset_version().with_parent_name(\"test dataset name\")\r\n",
    "  \r\n",
    "  run.add_outputs([train_ds_version,test_ds_version])\r\n"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "id": "y0NsPUJR209T"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our data contains now the same number of zeros and ones now"
   ],
   "metadata": {
    "id": "16e133e5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get different user versions\r\n"
   ],
   "metadata": {
    "id": "4a071e3a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a random user version by calling get_random_string"
   ],
   "metadata": {
    "id": "e09c73d9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's generate some unique names for our following modeling experiments\r\n",
    "import random\r\n",
    "import string\r\n",
    "def get_random_string(length):\r\n",
    "    return \"\".join(random.choice(string.ascii_letters) for i in range(length))"
   ],
   "outputs": [],
   "metadata": {
    "id": "a08ba1f5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling\r\n",
    "* LogisticRegression\r\n",
    "* LightGBM Classification"
   ],
   "metadata": {
    "id": "d42f694c",
    "papermill": {
     "duration": 0.048004,
     "end_time": "2021-04-10T09:30:32.545240",
     "exception": false,
     "start_time": "2021-04-10T09:30:32.497236",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we create a function that calculates and shows the confusion matrix and the accuracy, precision, recall, f1_score, roc_auc metrics.\r\n",
    "\r\n",
    "- Confusion matrix: Confusion matrices represent counts from predicted and actual values. It shows the rates of TP, FP, FN and TN\r\n",
    "- Accuracy: The model’s capability to correctly predict both the positives and negatives out of all the predictions. Accuracy_score =  (TP + TN)/ (TP + FN + TN + FP)\r\n",
    "- Precision: The model's capability to correctly predict the positives out of all the positive prediction it made. Precision Score = TP / (FP + TP)\r\n",
    "- Recall: The model’s capability to correctly predict the positives out of actual positives. This is unlike precision which measures as to how many predictions made by models are actually positive out of all positive predictions made. Recall score is a useful measure of success of prediction when the classes are very imbalanced. Recall Score = TP / (FN + TP)\r\n",
    "- F1score: The model score as a function of precision and recall score. This is useful measure of the model in the scenarios where one tries to optimize either of precision or recall score and as a result, the model performance suffers.F1_Score = 2* Precision Score * Recall Score/ (Precision Score + Recall Score)\r\n",
    "- roc_auc_score : ROC curve is a graph that shows the performance of a classification model at all possible thresholds( threshold is a particular value beyond which you say a point belongs to a particular class). AUC measures how well a model is able to distinguish between classes. The curve is plotted between two parameters :\r\n",
    "    * TRUE POSITIVE RATE\r\n",
    "    * FALSE POSTIVIE RATE\r\n",
    "- True Positive (TP): The value of correct predictions of positives out of actual positive cases. Ex : Predict a well person as not sick\r\n",
    "- False Positive (FP): The value of incorrect positive predictions. The number of negatives falsly pridected as positives. Ex : Predict a sick person as not sick\r\n",
    "- True Negative (TN): The value of correct predictions of negatives out of actual negative cases. Ex : Predict a sick person as sick\r\n",
    "- False Negative (FN): The value of incorrect negative predictions. This value represents the number of positives which gets falsely. Predict a well person as sick"
   ],
   "metadata": {
    "id": "76a140a5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_clf_eval(y_test, pred = None, pred_proba = None):\r\n",
    "    confusion = confusion_matrix(y_test, pred)\r\n",
    "    accuracy = accuracy_score(y_test, pred)\r\n",
    "    precision = precision_score(y_test, pred)\r\n",
    "    recall = recall_score(y_test, pred)\r\n",
    "    f1 = f1_score(y_test, pred)\r\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\r\n",
    "    \r\n",
    "    print('confusion')\r\n",
    "    print(confusion)\r\n",
    "    print('Accuacy : {}'.format(np.around(accuracy,4)))\r\n",
    "    print('Precision: {}'.format(np.around(precision,4)))\r\n",
    "    print('Recall : {}'.format(np.around(recall,4)))\r\n",
    "    print('F1 : {}'.format(np.around(f1,4)))  \r\n",
    "    print('ROC_AUC : {}'.format(np.around(roc_auc,4)))\r\n",
    "    return confusion, accuracy, precision, recall, f1, roc_auc"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:32.647997Z",
     "iopub.status.busy": "2021-04-10T09:30:32.647351Z",
     "iopub.status.idle": "2021-04-10T09:30:32.652002Z",
     "shell.execute_reply": "2021-04-10T09:30:32.651414Z"
    },
    "id": "afef1776",
    "papermill": {
     "duration": 0.05953,
     "end_time": "2021-04-10T09:30:32.652140",
     "exception": false,
     "start_time": "2021-04-10T09:30:32.592610",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **LogisticRegression**\r\n",
    "\r\n",
    "[Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) is used to calculate the probability of a binary event occurring. For example, predicting if a credit card transaction is fraudulent or not fraudulent or predicting if an incoming email is spam or not spam"
   ],
   "metadata": {
    "id": "2f5f2f7a",
    "papermill": {
     "duration": 0.047644,
     "end_time": "2021-04-10T09:30:32.748450",
     "exception": false,
     "start_time": "2021-04-10T09:30:32.700806",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code creates a Logistic regression model and calculates the metrics related to this model. Complete the code by adding a job run to create a model and send the metrics to Vectice (You can look at the examples in the documentation) and don't forget to use the names you generated for your experimints"
   ],
   "metadata": {
    "id": "z8X87UoraqYR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Logistic Regression\r\n",
    "## Create a run\r\n",
    "##Start the run\r\n",
    "lg_reg = LogisticRegression()\r\n",
    "lg_reg.fit(x_over, y_over)\r\n",
    "pred = lg_reg.predict(scaler_x_test)\r\n",
    "pred_proba = lg_reg.predict_proba(scaler_x_test)[:,1]\r\n",
    "\r\n",
    "confusion, accuracy, precision, recall, f1, roc_auc = get_clf_eval(scaler_y_test, pred=pred, pred_proba=pred_proba)\r\n",
    "\r\n",
    "## Create a model version and add metrics and properties to it (you can use the function get_random_string defined below in order to be able to generate different user versions)\r\n",
    "  "
   ],
   "outputs": [],
   "metadata": {
    "id": "8abc4d92"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Double click to show the answer\r\n",
    "\r\n",
    "uri = \"https://github.com/vectice/vectice-examples\"\r\n",
    "script_relative_path=\"Notebooks/Vanilla/Customer_satisfaction_example/customer_satisfaction_challenge.ipynb\"\r\n",
    "input_code = Vectice.create_code_version_with_github_uri(uri=uri, script_relative_path=script_relative_path)\r\n",
    "vectice.create_run(job_name = \"Train model with Logistic regression\", job_type = JobType.TRAINING)\r\n",
    "\r\n",
    "with vectice.start_run(inputs=[train_ds_version,test_ds_version, input_code]) as run:\r\n",
    "  lg_reg = LogisticRegression()\r\n",
    "\r\n",
    "  lg_reg.fit(x_over, y_over)\r\n",
    "  pred = lg_reg.predict(scaler_x_test)\r\n",
    "  pred_proba = lg_reg.predict_proba(scaler_x_test)[:,1]\r\n",
    "\r\n",
    "  confusion, accuracy, precision, recall, f1, roc_auc = get_clf_eval(scaler_y_test, pred=pred, pred_proba=pred_proba)\r\n",
    "      \r\n",
    "  metrics = [('Accuracy score', accuracy), (\"Precision,\",precision), (\"Recall\", recall), ('f1 score', f1), ('ROC_AUC', roc_auc)]\r\n",
    "\r\n",
    "  model_version1 = vectice.create_model_version().with_parent_name(\"logisticRegression\").with_algorithm(\"Classification : Logistic Regression\").with_metrics(metrics).with_user_version(get_random_string(12))\r\n",
    "  run.add_outputs([model_version1])"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "id": "25VSc06-abyD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **LightGBM Classifier**\r\n",
    "\r\n",
    "[LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html) is a gradient boosting classifier in machine learning that uses tree-based learning algorithms. It can handle a large amount of data, less memory usage, has parallel and GPU learning, good accuracy, faster training speed and efficiency"
   ],
   "metadata": {
    "id": "4b30a479",
    "papermill": {
     "duration": 0.048212,
     "end_time": "2021-04-10T09:30:40.141181",
     "exception": false,
     "start_time": "2021-04-10T09:30:40.092969",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler_x_test, scaler_x_val, scaler_y_test, scaler_y_val = train_test_split(scaler_x_test, scaler_y_test, test_size=0.5)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:40.247991Z",
     "iopub.status.busy": "2021-04-10T09:30:40.246574Z",
     "iopub.status.idle": "2021-04-10T09:30:40.269645Z",
     "shell.execute_reply": "2021-04-10T09:30:40.270257Z"
    },
    "id": "e1746651",
    "papermill": {
     "duration": 0.079436,
     "end_time": "2021-04-10T09:30:40.270446",
     "exception": false,
     "start_time": "2021-04-10T09:30:40.191010",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##Setting up the model's parameters\r\n",
    "## Feel free to play with the parameters\r\n",
    "train_data = lgb.Dataset(x_over, label=y_over)\r\n",
    "val_data = lgb.Dataset(scaler_x_val, label=scaler_y_val)\r\n",
    "n_estimators = 5000\r\n",
    "num_leaves = 20\r\n",
    "max_depth = -1\r\n",
    "min_data_in_leaf = 80\r\n",
    "learning_rate = 0.001\r\n",
    "boosting = 'gbdt'\r\n",
    "objective = 'binary'\r\n",
    "metric = 'auc'\r\n",
    "n_jobs = -1\r\n",
    "params = {\r\n",
    "    'n_estimators': n_estimators,\r\n",
    "    'num_leaves': num_leaves,\r\n",
    "    'max_depth': max_depth,\r\n",
    "    'min_data_in_leaf': min_data_in_leaf,\r\n",
    "    'learning_rate': learning_rate,\r\n",
    "    'boosting': boosting,\r\n",
    "    'objective': objective,\r\n",
    "    'metric': metric,\r\n",
    "    'n_jobs': n_jobs\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-10T09:30:40.373748Z",
     "iopub.status.busy": "2021-04-10T09:30:40.372987Z",
     "iopub.status.idle": "2021-04-10T09:30:40.377244Z",
     "shell.execute_reply": "2021-04-10T09:30:40.376801Z"
    },
    "id": "f73fc68d",
    "papermill": {
     "duration": 0.058534,
     "end_time": "2021-04-10T09:30:40.377387",
     "exception": false,
     "start_time": "2021-04-10T09:30:40.318853",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code creates a LightGBM classifier model and calculates the metrics related to this model. Complete the code by adding a job run to create a model and send the metrics to Vectice and don't forget to give the names dataset version you created before as inputs to you model"
   ],
   "metadata": {
    "id": "FAxn52vcawZl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Create a run\r\n",
    "##Start the run\r\n",
    "lgbm = lgb.train(params,\r\n",
    "                  train_data,\r\n",
    "                  valid_sets=val_data, \r\n",
    "                  valid_names=['train','valid'],\r\n",
    "                  early_stopping_rounds=300)\r\n",
    "\r\n",
    "# Predicting the output on the Test Dataset \r\n",
    "ypred_lgbm = lgbm.predict(scaler_x_test)\r\n",
    "ypred_lgbm\r\n",
    "y_pred_lgbm_class = [np.argmax(line) for line in ypred_lgbm]\r\n",
    "accuracy_lgbm=accuracy_score(scaler_y_test,y_pred_lgbm_class)\r\n",
    "print(accuracy_lgbm)\r\n",
    "#Print Area Under Curve\r\n",
    "plt.figure()\r\n",
    "false_positive_rate, recall, thresholds = roc_curve(scaler_y_test, ypred_lgbm)\r\n",
    "roc_auc = auc(false_positive_rate, recall)\r\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\r\n",
    "plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\r\n",
    "plt.legend(loc='lower right')\r\n",
    "plt.plot([0,1], [0,1], 'r--')\r\n",
    "plt.xlim([0.0,1.0])\r\n",
    "plt.ylim([0.0,1.0])\r\n",
    "plt.ylabel('Recall')\r\n",
    "plt.xlabel('Fall-out (1-Specificity)')\r\n",
    "plt.show()\r\n",
    "print('AUC score:', roc_auc)\r\n",
    "\r\n",
    "## Create a model version and add metrics and properties to it (you can use the function get_random_string defined below in order to be able to generate different user versions)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "41a28b98"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Double click to show the answer\r\n",
    "\r\n",
    "uri = \"https://github.com/vectice/vectice-examples\"\r\n",
    "script_relative_path=\"Notebooks/Vanilla/Customer_satisfaction_example/customer_satisfaction_challenge.ipynb\"\r\n",
    "input_code = Vectice.create_code_version_with_github_uri(uri=uri, script_relative_path=script_relative_path)\r\n",
    "vectice.create_run(job_name=\"Train model with lightgbm\", job_type = JobType.TRAINING)\r\n",
    "\r\n",
    "with vectice.start_run(inputs=[train_ds_version,test_ds_version, input_code]) as run:\r\n",
    "\r\n",
    "  lgbm = lgb.train(params,\r\n",
    "                    train_data,\r\n",
    "                    valid_sets=val_data, \r\n",
    "                    valid_names=['train','valid'],\r\n",
    "                    early_stopping_rounds=300)\r\n",
    "\r\n",
    "  # Predicting the output on the Test Dataset \r\n",
    "  ypred_lgbm = lgbm.predict(scaler_x_test)\r\n",
    "  ypred_lgbm\r\n",
    "  y_pred_lgbm_class = [np.argmax(line) for line in ypred_lgbm]\r\n",
    "  accuracy_lgbm=accuracy_score(scaler_y_test,y_pred_lgbm_class)\r\n",
    "  print(accuracy_lgbm)\r\n",
    "  #Print Area Under Curve\r\n",
    "  plt.figure()\r\n",
    "  false_positive_rate, recall, thresholds = roc_curve(scaler_y_test, ypred_lgbm)\r\n",
    "  roc_auc = auc(false_positive_rate, recall)\r\n",
    "  plt.title('Receiver Operating Characteristic (ROC)')\r\n",
    "  plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\r\n",
    "  plt.legend(loc='lower right')\r\n",
    "  plt.plot([0,1], [0,1], 'r--')\r\n",
    "  plt.xlim([0.0,1.0])\r\n",
    "  plt.ylim([0.0,1.0])\r\n",
    "  plt.ylabel('Recall')\r\n",
    "  plt.xlabel('Fall-out (1-Specificity)')\r\n",
    "  plt.show()\r\n",
    "  print('AUC score:', roc_auc)\r\n",
    "\r\n",
    "  metrics =[(\"Accuracy\", accuracy_lgbm), (\"AUC score\", roc_auc)] \r\n",
    "  properties =  [(\"n_estimators\", str(n_estimators)), (\"num_leaves\", str(num_leaves)), (\"max_depth\", str(max_depth)),\r\n",
    "                (\"min_data_in_leaf\", str(min_data_in_leaf)), (\"learning_rate\", str(learning_rate)), (\"boosting\", str(boosting)),\r\n",
    "                (\"objective\", str(objective)), (\"metric\", str(metric)), (\"n_jobs\", str(n_jobs))]\r\n",
    "\r\n",
    "  model_version2 = vectice.create_model_version().with_parent_name(\"LGBMclassification\").with_algorithm(\"Classification : LGBM\").with_properties(properties).with_metrics(metrics).with_user_version(get_random_string(12))\r\n",
    "  run.add_outputs([model_version2])"
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "id": "cLn__vFAbYfK"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eb29db7c",
    "e786cc41"
   ],
   "include_colab_link": true,
   "name": "Customer_satisfaction_challenge.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "7eb6f6862a6947ac5369bdb367f27d42e7da01a01c09c3d67648c4373e3c8da1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 162.044116,
   "end_time": "2021-04-10T09:32:31.418407",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-10T09:29:49.374291",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}